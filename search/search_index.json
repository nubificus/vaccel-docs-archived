{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to vAccel vAccel is a framework offering hardware acceleration primitive with focus on portability. It exposes to programmers \"accelerate-able\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. QEMU and AWS Firecracker VMMs are currently supported compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale. vAccel design vAccel runtime stack The core component of vAccel is the vaccel runtime (vAccelRT). vAccelRT is designed in a modular way. The core runtime exposes the vAccel API to user applications and dispatches requests to one of many backend plugins , which are the components that implement the vAccel API on a particular hardware accelerator. The user application links against the core runtime library and the plugin modules are loaded at runtime by the runtime. This workflow decouples the application from the hardware accelerator-specific parts of the stack, allowing for seamless migration of the same binary to different platforms with different accelerator capabilities, without the need to recomplile user code. Currently, we support acceleration plugins for: Jetson inference framework, for acceleration of ML operation on Nvidia GPUs. Google Coral TPU , for acceleration on Coral TPUs. Hardware acceleration in Virtual Machines Hardware acceleration for virtualized guests is a difficult problem to tackle. Typical solutions involve device pass-through or paravirtual drivers that expose hardware semantics inside the guest. vAccel differentiates itself from these approaches by exposing coarse grain \"acceleratable\" functions in the guest over a custom transport layer. The semantics of the transport layer are hidden from the programmer. A vAccel application that runs on baremetal with an Nvidia GPU can run as is inside a VM using our appropriate VirtIO backend plugin. We have implemented the necessary parts for our VirtIO driver in our forks of QEMU and Firecracker hypervisors. Performance vAccel performance overhead of VM execution compared to bare-metal The above figure depicts a performance comparison of the image classification operation of a vAccel application running inside a Firecracker VM using the Jetson inference plugin comparing to an execution of the same operation using the Jetson inference framework natively without any vAccel code, running directly on the host. The performance overhead of our stack is less that 5% of the native execution time across a range of image sizes. We will be updating these performance results regularly with more operations and plugins.","title":"Home"},{"location":"#welcome-to-vaccel","text":"vAccel is a framework offering hardware acceleration primitive with focus on portability. It exposes to programmers \"accelerate-able\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. QEMU and AWS Firecracker VMMs are currently supported compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale.","title":"Welcome to vAccel"},{"location":"#vaccel-design","text":"vAccel runtime stack The core component of vAccel is the vaccel runtime (vAccelRT). vAccelRT is designed in a modular way. The core runtime exposes the vAccel API to user applications and dispatches requests to one of many backend plugins , which are the components that implement the vAccel API on a particular hardware accelerator. The user application links against the core runtime library and the plugin modules are loaded at runtime by the runtime. This workflow decouples the application from the hardware accelerator-specific parts of the stack, allowing for seamless migration of the same binary to different platforms with different accelerator capabilities, without the need to recomplile user code. Currently, we support acceleration plugins for: Jetson inference framework, for acceleration of ML operation on Nvidia GPUs. Google Coral TPU , for acceleration on Coral TPUs.","title":"vAccel design"},{"location":"#hardware-acceleration-in-virtual-machines","text":"Hardware acceleration for virtualized guests is a difficult problem to tackle. Typical solutions involve device pass-through or paravirtual drivers that expose hardware semantics inside the guest. vAccel differentiates itself from these approaches by exposing coarse grain \"acceleratable\" functions in the guest over a custom transport layer. The semantics of the transport layer are hidden from the programmer. A vAccel application that runs on baremetal with an Nvidia GPU can run as is inside a VM using our appropriate VirtIO backend plugin. We have implemented the necessary parts for our VirtIO driver in our forks of QEMU and Firecracker hypervisors.","title":"Hardware acceleration in Virtual Machines"},{"location":"#performance","text":"vAccel performance overhead of VM execution compared to bare-metal The above figure depicts a performance comparison of the image classification operation of a vAccel application running inside a Firecracker VM using the Jetson inference plugin comparing to an execution of the same operation using the Jetson inference framework natively without any vAccel code, running directly on the host. The performance overhead of our stack is less that 5% of the native execution time across a range of image sizes. We will be updating these performance results regularly with more operations and plugins.","title":"Performance"},{"location":"api/","text":"vAccel API (WiP) The vAccel API is work in progress. This page presents the current form of the vAccel frontend API, that is what is presented to the user. Using this API users can build vAccel applications that interface directly with libvaccel.so and their operations can be executed using vAccel plugins. Image inference vAccel supports a set of operations for Image inference using the following primitive functions (per operation): Image classification int vaccel_image_classification(struct vaccel_session *sess, const void *img, unsigned char *out_text, unsigned char *out_imgname, size_t len_img, size_t len_out_text, size_t len_out_imgname); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() ; const void *img : the buffer holding the data to the input image. unsigned char *out_text : the buffer holding the classification tag output unsigned char *out_imgname : the name of the processed image, created as a session resource (EXPERIMENTAL). size_t len_img : the size of img in bytes. size_t len_out_text : the size of out_text in bytes. size_t len_out_imgname : the size of out_imagename in bytes. Image segmentation int vaccel_image_segmentation(struct vaccel_session *sess, const void *img, const unsigned char *out_imgname, size_t len_img, size_t len_out_imgname); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() ; const void *img : the buffer holding the data to the input image. unsigned char *out_imgname : the name of the processed image, created as a session resource (EXPERIMENTAL). size_t len_img : the size of img in bytes. size_t len_out_imgname : the size of out_imagename in bytes. Object detection int vaccel_image_detection(struct vaccel_session *sess, const void *img, const unsigned char *out_imgname, size_t len_img, size_t len_out_imgname); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() ; const void *img : the buffer holding the data to the input image. unsigned char *out_imgname : the name of the processed image, created as a session resource (EXPERIMENTAL). size_t len_img : the size of img in bytes. size_t len_out_imgname : the size of out_imagename in bytes. BLAS library functions Matrix-to-matrix multiplication int vaccel_sgemm(struct vaccel_session *sess, uint32_t k, uint32_t m, uint32_t n, size_t len_a, size_t len_b, size_t len_c, float *a, float *b, float *c); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() . uint32_t k : first dimension of matrix A & matrix C. uint32_t l : second dimension of matrix A & first dimension of matrix B. uint32_t n : second dimension of matrix B & matrix C. size_t len_a : size of matrix A in bytes. size_t len_b : size of matrix B in bytes. size_t len_c : size of matrix C in bytes. float *a : pointer to matrix A. float *b : pointer to matrix B. float *c : pointer to matrix C. Generic functions Noop int vaccel_noop(struct vaccel_session *sess); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() . Exec int vaccel_exec(struct vaccel_session *sess, const char *library, const char *fn_symbol, struct vaccel_arg *read, size_t nr_read, struct vaccel_arg *write, size_t nr_write); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() . const char *library : name of the shared object to open. const char *fn_symbol : name of the symbol to dereference in the shared object library . struct vaccel_arg *read : pointer to an array of struct vaccel_arg read-only arguments to fn_symbol . size_t nr_read : number of elements for the read array. struct vaccel_arg *write : pointer to an array of struct vaccel_arg write-only arguments to fn_symbol . size_t nr_write : number of elements for the write array.","title":"vAccel API (WiP)"},{"location":"api/#vaccel-api-wip","text":"The vAccel API is work in progress. This page presents the current form of the vAccel frontend API, that is what is presented to the user. Using this API users can build vAccel applications that interface directly with libvaccel.so and their operations can be executed using vAccel plugins.","title":"vAccel API (WiP)"},{"location":"api/#image-inference","text":"vAccel supports a set of operations for Image inference using the following primitive functions (per operation):","title":"Image inference"},{"location":"api/#image-classification","text":"int vaccel_image_classification(struct vaccel_session *sess, const void *img, unsigned char *out_text, unsigned char *out_imgname, size_t len_img, size_t len_out_text, size_t len_out_imgname); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() ; const void *img : the buffer holding the data to the input image. unsigned char *out_text : the buffer holding the classification tag output unsigned char *out_imgname : the name of the processed image, created as a session resource (EXPERIMENTAL). size_t len_img : the size of img in bytes. size_t len_out_text : the size of out_text in bytes. size_t len_out_imgname : the size of out_imagename in bytes.","title":"Image classification"},{"location":"api/#image-segmentation","text":"int vaccel_image_segmentation(struct vaccel_session *sess, const void *img, const unsigned char *out_imgname, size_t len_img, size_t len_out_imgname); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() ; const void *img : the buffer holding the data to the input image. unsigned char *out_imgname : the name of the processed image, created as a session resource (EXPERIMENTAL). size_t len_img : the size of img in bytes. size_t len_out_imgname : the size of out_imagename in bytes.","title":"Image segmentation"},{"location":"api/#object-detection","text":"int vaccel_image_detection(struct vaccel_session *sess, const void *img, const unsigned char *out_imgname, size_t len_img, size_t len_out_imgname); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() ; const void *img : the buffer holding the data to the input image. unsigned char *out_imgname : the name of the processed image, created as a session resource (EXPERIMENTAL). size_t len_img : the size of img in bytes. size_t len_out_imgname : the size of out_imagename in bytes.","title":"Object detection"},{"location":"api/#blas-library-functions","text":"","title":"BLAS library functions"},{"location":"api/#matrix-to-matrix-multiplication","text":"int vaccel_sgemm(struct vaccel_session *sess, uint32_t k, uint32_t m, uint32_t n, size_t len_a, size_t len_b, size_t len_c, float *a, float *b, float *c); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() . uint32_t k : first dimension of matrix A & matrix C. uint32_t l : second dimension of matrix A & first dimension of matrix B. uint32_t n : second dimension of matrix B & matrix C. size_t len_a : size of matrix A in bytes. size_t len_b : size of matrix B in bytes. size_t len_c : size of matrix C in bytes. float *a : pointer to matrix A. float *b : pointer to matrix B. float *c : pointer to matrix C.","title":"Matrix-to-matrix multiplication"},{"location":"api/#generic-functions","text":"","title":"Generic functions"},{"location":"api/#noop","text":"int vaccel_noop(struct vaccel_session *sess); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() .","title":"Noop"},{"location":"api/#exec","text":"int vaccel_exec(struct vaccel_session *sess, const char *library, const char *fn_symbol, struct vaccel_arg *read, size_t nr_read, struct vaccel_arg *write, size_t nr_write); struct vaccel_session *sess : a pointer to a vAccel session created using vaccel_sess_init() . const char *library : name of the shared object to open. const char *fn_symbol : name of the symbol to dereference in the shared object library . struct vaccel_arg *read : pointer to an array of struct vaccel_arg read-only arguments to fn_symbol . size_t nr_read : number of elements for the read array. struct vaccel_arg *write : pointer to an array of struct vaccel_arg write-only arguments to fn_symbol . size_t nr_write : number of elements for the write array.","title":"Exec"},{"location":"coral/","text":"Google Coral TPU To build the vAccel Google Coral TPU plugin, we will use a simple image classification example based on tflite To execute an image classification operation on the Google Coral TPU, we first need to implement the operation using one of the available acceleration frameworks and then register this function to the vAccel plugin we are building. To achieve this, we patch a tflite example for Google coral, and build the available image classification function as a shared library, exposing the following API to the relevant plugin: int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) Patch the code The first step in order to build the library is to get the example code from the Google Coral github repo: git clone https://github.com/google-coral/tflite Then we need to apply the following patch ( libify_classify.patch ) to the relevant example: diff --git a/cpp/examples/classification/classify.cc b/cpp/examples/classification/classify.cc index 69dfe04..1129976 100644 --- a/cpp/examples/classification/classify.cc +++ b/cpp/examples/classification/classify.cc @@ -22,6 +22,72 @@ int32_t ToInt32(const char p[4]) { return (p[3] << 24) | (p[2] << 16) | (p[1] << 8) | p[0]; } +std::vector<uint8_t> ReadBmpImageFromBuf(char* buf, + int* out_width = nullptr, + int* out_height = nullptr, + int* out_channels = nullptr) { + char header[kBmpHeaderSize]; + memcpy(header, buf, sizeof(header)); + + const char* file_header = header; + const char* info_header = header + kBmpFileHeaderSize; + char *ptr = buf + sizeof(header); + + if (file_header[0] != 'B' || file_header[1] != 'M') + return {}; // Invalid file type. + + const int channels = info_header[14] / 8; + if (channels != 1 && channels != 3) return {}; // Unsupported bits per pixel. + + if (ToInt32(&info_header[16]) != 0) return {}; // Unsupported compression. + + uint32_t offset = ToInt32(&file_header[10]); +#if 0 + if (offset > kBmpHeaderSize)// && + //!file.seekg(offset - kBmpHeaderSize, std::ios::cur)) + return {}; // Seek failed. +#endif + + offset -= kBmpHeaderSize; + //printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + //printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + //printf(\"offset:%d\\n\", offset); + ptr = ptr+offset; + int width = ToInt32(&info_header[4]); + if (width < 0) return {}; // Invalid width. + + int height = ToInt32(&info_header[8]); + const bool top_down = height < 0; + if (top_down) height = -height; + + const int line_bytes = width * channels; + const int line_padding_bytes = + 4 * ((8 * channels * width + 31) / 32) - line_bytes; + std::vector<uint8_t> image(line_bytes * height); + for (int i = 0; i < height; ++i) { + uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + memcpy(line, ptr, line_bytes); +#if 0 + if (!file.read(reinterpret_cast<char*>(line), line_bytes)) + return {}; // Read failed. + if (!file.seekg(line_padding_bytes, std::ios::cur)) + return {}; // Seek failed. +#endif + //printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); + ptr = ptr + line_padding_bytes; + ptr = ptr + line_bytes; + if (channels == 3) { + for (int j = 0; j < width; ++j) std::swap(line[3 * j], line[3 * j + 2]); + } + } + + if (out_width) *out_width = width; + if (out_height) *out_height = height; + if (out_channels) *out_channels = channels; + return image; +} + + std::vector<uint8_t> ReadBmpImage(const char* filename, int* out_width = nullptr, int* out_height = nullptr, @@ -50,6 +116,9 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, !file.seekg(offset - kBmpHeaderSize, std::ios::cur)) return {}; // Seek failed. + printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + printf(\"offset:%d\\n\", offset); int width = ToInt32(&info_header[4]); if (width < 0) return {}; // Invalid width. @@ -63,6 +132,7 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, std::vector<uint8_t> image(line_bytes * height); for (int i = 0; i < height; ++i) { uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); if (!file.read(reinterpret_cast<char*>(line), line_bytes)) return {}; // Read failed. if (!file.seekg(line_padding_bytes, std::ios::cur)) @@ -117,18 +187,12 @@ std::vector<std::pair<int, float>> Sort(const std::vector<float>& scores, } } // namespace -int main(int argc, char* argv[]) { - if (argc != 5) { - std::cerr << argv[0] - << \" <model_file> <label_file> <image_file> <threshold>\" - << std::endl; - return 1; - } - - const std::string model_file = argv[1]; - const std::string label_file = argv[2]; - const std::string image_file = argv[3]; - const float threshold = std::stof(argv[4]); +extern \"C\" int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) +{ + const std::string model_file (model_file_ptr); + const std::string label_file (label_file_ptr); + const std::string image_file (image_file_ptr); + const float threshold = thres; // Find TPU device. size_t num_devices; @@ -151,7 +215,8 @@ int main(int argc, char* argv[]) { // Load image. int image_bpp, image_width, image_height; auto image = - ReadBmpImage(image_file.c_str(), &image_width, &image_height, &image_bpp); + ReadBmpImageFromBuf(image_file_ptr, &image_width, &image_height, &image_bpp); + //ReadBmpImage(image_file_ptr, &image_width, &image_height, &image_bpp); if (image.empty()) { std::cerr << \"Cannot read image from \" << image_file << std::endl; return 1; @@ -174,7 +239,7 @@ int main(int argc, char* argv[]) { auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0); - interpreter->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate}); + interpreter->ModifyGraphWithDelegate(delegate); // Allocate tensors. if (interpreter->AllocateTensors() != kTfLiteOk) { @@ -204,9 +269,13 @@ int main(int argc, char* argv[]) { // Get interpreter output. auto results = Sort(Dequantize(*interpreter->output_tensor(0)), threshold); + *tags = strdup(GetLabel(labels, results[0].first).c_str()); + *out_len = strlen(*tags); +#if 0 for (auto& result : results) std::cout << std::setw(7) << std::fixed << std::setprecision(5) << result.second << GetLabel(labels, result.first) << std::endl; +#endif return 0; } cd tflite patch -p1 < ../libify_classify.path Alternatively you could just clone from our forked repo: git clone https://github.com/nubificus/tflite -b vaccel Then, we need to install the requirements & build tflite for google coral (could take some time, so please be patient ;-)). Download the latest Edge TPU runtime You can grab the latest Edge TPU runtime archive from https://coral.ai/software/ . Then extract it next to the Makefile: cd tflite/cpp/examples/classification/ wget https://dl.google.com/coral/edgetpu_api/edgetpu_runtime_20200710.zip Download and build Tensorflow git clone https://github.com/tensorflow/tensorflow.git tensorflow/tensorflow/lite/tools/make/download_dependencies.sh tensorflow/tensorflow/lite/tools/make/build_lib.sh Build the shared library Finally, build the classify shared object that we will then link to the vAccel Google Coral TPU plugin. g++ -shared -fPIC -std=c++11 classify.cc -o /usr/local/lib/libclassify.so -I/data/tflite/cpp/examples/classification/edgetpu_runtime/libedgetpu/ -Itensorflow/ -Itensorflow//tensorflow/lite/tools/make/downloads/flatbuffers/include -Ltensorflow//tensorflow/lite/tools/make/gen/generic-aarch64_armv8-a/lib -Ledgetpu_runtime/libedgetpu/direct/aarch64/ -l:libedgetpu.so.1.0 -lpthread -lm -ldl tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a Build vAccelRT plugin Following the generic process for vAccelRT building (from Building vAccelRT ) we now have to enable the vAccel Google Coral Edge TPU plugin. In short, the previous steps for vAccelRT were the following 1 : git clone --recursive https://github.com/cloudkernels/vaccelrt -b feat_gcoral_plugin cd vaccelrt mkdir build cd build cmake ../ The additional step now is to enable the plugin via cmake: cd vaccelrt mkdir build_coral && cd build_coral cmake ../ -DBUILD_PLUGIN_CORAL=ON The output of the above command should be something like the following: -- The C compiler identification is GNU 8.3.0 -- The CXX compiler identification is GNU 8.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral/googletest-download Scanning dependencies of target googletest [ 11%] Creating directories for 'googletest' [ 22%] Performing download step (git clone) for 'googletest' Cloning into 'googletest-src'... Note: checking out 'release-1.8.1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at 2fe3bd99 Merge pull request #1433 from dsacre/fix-clang-warnings [ 33%] No patch step for 'googletest' [ 44%] Performing update step for 'googletest' [ 55%] No configure step for 'googletest' [ 66%] No build step for 'googletest' [ 77%] No install step for 'googletest' [ 88%] No test step for 'googletest' [100%] Completed 'googletest' [100%] Built target googletest -- Found PythonInterp: /usr/bin/python (found version \"2.7.16\") -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Found rt: /usr/lib/aarch64-linux-gnu/librt.so Include directories: /data/vaccelrt/src -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral Finally, building the plugin is as simple as issueing a make command: make The plugin is available at: vaccelrt/build_coral/plugins/coral/libvaccel-coral.so Extra Files To run an example on the Google Coral Edge TPU, follow the instructions available at Running a simple example . The files needed for classification are: mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt available for download from: https://github.com/google-coral/test_data . Support for Google Coral is WiP , and will be available shortly in the main branch. \u21a9","title":"Google Coral TPU"},{"location":"coral/#google-coral-tpu","text":"To build the vAccel Google Coral TPU plugin, we will use a simple image classification example based on tflite To execute an image classification operation on the Google Coral TPU, we first need to implement the operation using one of the available acceleration frameworks and then register this function to the vAccel plugin we are building. To achieve this, we patch a tflite example for Google coral, and build the available image classification function as a shared library, exposing the following API to the relevant plugin: int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len)","title":"Google Coral TPU"},{"location":"coral/#patch-the-code","text":"The first step in order to build the library is to get the example code from the Google Coral github repo: git clone https://github.com/google-coral/tflite Then we need to apply the following patch ( libify_classify.patch ) to the relevant example: diff --git a/cpp/examples/classification/classify.cc b/cpp/examples/classification/classify.cc index 69dfe04..1129976 100644 --- a/cpp/examples/classification/classify.cc +++ b/cpp/examples/classification/classify.cc @@ -22,6 +22,72 @@ int32_t ToInt32(const char p[4]) { return (p[3] << 24) | (p[2] << 16) | (p[1] << 8) | p[0]; } +std::vector<uint8_t> ReadBmpImageFromBuf(char* buf, + int* out_width = nullptr, + int* out_height = nullptr, + int* out_channels = nullptr) { + char header[kBmpHeaderSize]; + memcpy(header, buf, sizeof(header)); + + const char* file_header = header; + const char* info_header = header + kBmpFileHeaderSize; + char *ptr = buf + sizeof(header); + + if (file_header[0] != 'B' || file_header[1] != 'M') + return {}; // Invalid file type. + + const int channels = info_header[14] / 8; + if (channels != 1 && channels != 3) return {}; // Unsupported bits per pixel. + + if (ToInt32(&info_header[16]) != 0) return {}; // Unsupported compression. + + uint32_t offset = ToInt32(&file_header[10]); +#if 0 + if (offset > kBmpHeaderSize)// && + //!file.seekg(offset - kBmpHeaderSize, std::ios::cur)) + return {}; // Seek failed. +#endif + + offset -= kBmpHeaderSize; + //printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + //printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + //printf(\"offset:%d\\n\", offset); + ptr = ptr+offset; + int width = ToInt32(&info_header[4]); + if (width < 0) return {}; // Invalid width. + + int height = ToInt32(&info_header[8]); + const bool top_down = height < 0; + if (top_down) height = -height; + + const int line_bytes = width * channels; + const int line_padding_bytes = + 4 * ((8 * channels * width + 31) / 32) - line_bytes; + std::vector<uint8_t> image(line_bytes * height); + for (int i = 0; i < height; ++i) { + uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + memcpy(line, ptr, line_bytes); +#if 0 + if (!file.read(reinterpret_cast<char*>(line), line_bytes)) + return {}; // Read failed. + if (!file.seekg(line_padding_bytes, std::ios::cur)) + return {}; // Seek failed. +#endif + //printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); + ptr = ptr + line_padding_bytes; + ptr = ptr + line_bytes; + if (channels == 3) { + for (int j = 0; j < width; ++j) std::swap(line[3 * j], line[3 * j + 2]); + } + } + + if (out_width) *out_width = width; + if (out_height) *out_height = height; + if (out_channels) *out_channels = channels; + return image; +} + + std::vector<uint8_t> ReadBmpImage(const char* filename, int* out_width = nullptr, int* out_height = nullptr, @@ -50,6 +116,9 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, !file.seekg(offset - kBmpHeaderSize, std::ios::cur)) return {}; // Seek failed. + printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + printf(\"offset:%d\\n\", offset); int width = ToInt32(&info_header[4]); if (width < 0) return {}; // Invalid width. @@ -63,6 +132,7 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, std::vector<uint8_t> image(line_bytes * height); for (int i = 0; i < height; ++i) { uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); if (!file.read(reinterpret_cast<char*>(line), line_bytes)) return {}; // Read failed. if (!file.seekg(line_padding_bytes, std::ios::cur)) @@ -117,18 +187,12 @@ std::vector<std::pair<int, float>> Sort(const std::vector<float>& scores, } } // namespace -int main(int argc, char* argv[]) { - if (argc != 5) { - std::cerr << argv[0] - << \" <model_file> <label_file> <image_file> <threshold>\" - << std::endl; - return 1; - } - - const std::string model_file = argv[1]; - const std::string label_file = argv[2]; - const std::string image_file = argv[3]; - const float threshold = std::stof(argv[4]); +extern \"C\" int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) +{ + const std::string model_file (model_file_ptr); + const std::string label_file (label_file_ptr); + const std::string image_file (image_file_ptr); + const float threshold = thres; // Find TPU device. size_t num_devices; @@ -151,7 +215,8 @@ int main(int argc, char* argv[]) { // Load image. int image_bpp, image_width, image_height; auto image = - ReadBmpImage(image_file.c_str(), &image_width, &image_height, &image_bpp); + ReadBmpImageFromBuf(image_file_ptr, &image_width, &image_height, &image_bpp); + //ReadBmpImage(image_file_ptr, &image_width, &image_height, &image_bpp); if (image.empty()) { std::cerr << \"Cannot read image from \" << image_file << std::endl; return 1; @@ -174,7 +239,7 @@ int main(int argc, char* argv[]) { auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0); - interpreter->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate}); + interpreter->ModifyGraphWithDelegate(delegate); // Allocate tensors. if (interpreter->AllocateTensors() != kTfLiteOk) { @@ -204,9 +269,13 @@ int main(int argc, char* argv[]) { // Get interpreter output. auto results = Sort(Dequantize(*interpreter->output_tensor(0)), threshold); + *tags = strdup(GetLabel(labels, results[0].first).c_str()); + *out_len = strlen(*tags); +#if 0 for (auto& result : results) std::cout << std::setw(7) << std::fixed << std::setprecision(5) << result.second << GetLabel(labels, result.first) << std::endl; +#endif return 0; } cd tflite patch -p1 < ../libify_classify.path Alternatively you could just clone from our forked repo: git clone https://github.com/nubificus/tflite -b vaccel Then, we need to install the requirements & build tflite for google coral (could take some time, so please be patient ;-)).","title":"Patch the code"},{"location":"coral/#download-the-latest-edge-tpu-runtime","text":"You can grab the latest Edge TPU runtime archive from https://coral.ai/software/ . Then extract it next to the Makefile: cd tflite/cpp/examples/classification/ wget https://dl.google.com/coral/edgetpu_api/edgetpu_runtime_20200710.zip","title":"Download the latest Edge TPU runtime"},{"location":"coral/#download-and-build-tensorflow","text":"git clone https://github.com/tensorflow/tensorflow.git tensorflow/tensorflow/lite/tools/make/download_dependencies.sh tensorflow/tensorflow/lite/tools/make/build_lib.sh","title":"Download and build Tensorflow"},{"location":"coral/#build-the-shared-library","text":"Finally, build the classify shared object that we will then link to the vAccel Google Coral TPU plugin. g++ -shared -fPIC -std=c++11 classify.cc -o /usr/local/lib/libclassify.so -I/data/tflite/cpp/examples/classification/edgetpu_runtime/libedgetpu/ -Itensorflow/ -Itensorflow//tensorflow/lite/tools/make/downloads/flatbuffers/include -Ltensorflow//tensorflow/lite/tools/make/gen/generic-aarch64_armv8-a/lib -Ledgetpu_runtime/libedgetpu/direct/aarch64/ -l:libedgetpu.so.1.0 -lpthread -lm -ldl tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a","title":"Build the shared library"},{"location":"coral/#build-vaccelrt-plugin","text":"Following the generic process for vAccelRT building (from Building vAccelRT ) we now have to enable the vAccel Google Coral Edge TPU plugin. In short, the previous steps for vAccelRT were the following 1 : git clone --recursive https://github.com/cloudkernels/vaccelrt -b feat_gcoral_plugin cd vaccelrt mkdir build cd build cmake ../ The additional step now is to enable the plugin via cmake: cd vaccelrt mkdir build_coral && cd build_coral cmake ../ -DBUILD_PLUGIN_CORAL=ON The output of the above command should be something like the following: -- The C compiler identification is GNU 8.3.0 -- The CXX compiler identification is GNU 8.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral/googletest-download Scanning dependencies of target googletest [ 11%] Creating directories for 'googletest' [ 22%] Performing download step (git clone) for 'googletest' Cloning into 'googletest-src'... Note: checking out 'release-1.8.1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at 2fe3bd99 Merge pull request #1433 from dsacre/fix-clang-warnings [ 33%] No patch step for 'googletest' [ 44%] Performing update step for 'googletest' [ 55%] No configure step for 'googletest' [ 66%] No build step for 'googletest' [ 77%] No install step for 'googletest' [ 88%] No test step for 'googletest' [100%] Completed 'googletest' [100%] Built target googletest -- Found PythonInterp: /usr/bin/python (found version \"2.7.16\") -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Found rt: /usr/lib/aarch64-linux-gnu/librt.so Include directories: /data/vaccelrt/src -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral Finally, building the plugin is as simple as issueing a make command: make The plugin is available at: vaccelrt/build_coral/plugins/coral/libvaccel-coral.so","title":"Build vAccelRT plugin"},{"location":"coral/#extra-files","text":"To run an example on the Google Coral Edge TPU, follow the instructions available at Running a simple example . The files needed for classification are: mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt available for download from: https://github.com/google-coral/test_data . Support for Google Coral is WiP , and will be available shortly in the main branch. \u21a9","title":"Extra Files"},{"location":"devmapper/","text":"Configure contained CRI plugin with devicemapper Follow the documentation to configure containerd with devicemapper. Don't forget to configure devicemapper as the default CRI snapshotter; make sure that the following lines exist in your containerd config.toml: [plugins.cri.containerd] snapshotter = \"devmapper\" k3s k3s ships with a minimal version of containerd where the devicemapper plugin is not included. Additionally to the above, you also need to either configure k3s to use a different containerd binary or built k3s-containerd from source , patch it and eject it to /var/lib/rancher/k3s/data/current/bin . diff --git a/pkg/containerd/builtins_linux.go b/pkg/containerd/builtins_linux.go index 2b28979b15..7b9afcd2a8 100644 --- a/pkg/containerd/builtins_linux.go +++ b/pkg/containerd/builtins_linux.go @@ -25,4 +25,5 @@ import ( _ \"github.com/containerd/containerd/runtime/v2/runc/options\" _ \"github.com/containerd/containerd/snapshots/native\" _ \"github.com/containerd/containerd/snapshots/overlay\" + _ \"github.com/containerd/containerd/snapshots/devmapper\" ) diff --git a/vendor/modules.txt b/vendor/modules.txt index 3ac906547a..84b4691e12 100644 --- a/vendor/modules.txt +++ b/vendor/modules.txt @@ -301,6 +301,8 @@ github.com/containerd/containerd/services/snapshots github.com/containerd/containerd/services/tasks github.com/containerd/containerd/services/version github.com/containerd/containerd/snapshots +github.com/containerd/containerd/snapshots/devmapper +github.com/containerd/containerd/snapshots/devmapper/dmsetup github.com/containerd/containerd/snapshots/native github.com/containerd/containerd/snapshots/overlay github.com/containerd/containerd/snapshots/proxy The devicemapper pool should be placed to k3s path (e.g DATA_DIR=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.devmapper ) Restart k8s/k3s services on each node affected If you are already running k8s/k3s with a different containerd snapshotter, after configuring devmapper and restarting containerd, you probably need to restart kubelet/k3s services and make sure that all the containers running in k8s.io containerd namespace are now running using devicemapper.","title":"Devmapper"},{"location":"devmapper/#configure-contained-cri-plugin-with-devicemapper","text":"Follow the documentation to configure containerd with devicemapper. Don't forget to configure devicemapper as the default CRI snapshotter; make sure that the following lines exist in your containerd config.toml: [plugins.cri.containerd] snapshotter = \"devmapper\"","title":"Configure contained CRI plugin with devicemapper"},{"location":"devmapper/#k3s","text":"k3s ships with a minimal version of containerd where the devicemapper plugin is not included. Additionally to the above, you also need to either configure k3s to use a different containerd binary or built k3s-containerd from source , patch it and eject it to /var/lib/rancher/k3s/data/current/bin . diff --git a/pkg/containerd/builtins_linux.go b/pkg/containerd/builtins_linux.go index 2b28979b15..7b9afcd2a8 100644 --- a/pkg/containerd/builtins_linux.go +++ b/pkg/containerd/builtins_linux.go @@ -25,4 +25,5 @@ import ( _ \"github.com/containerd/containerd/runtime/v2/runc/options\" _ \"github.com/containerd/containerd/snapshots/native\" _ \"github.com/containerd/containerd/snapshots/overlay\" + _ \"github.com/containerd/containerd/snapshots/devmapper\" ) diff --git a/vendor/modules.txt b/vendor/modules.txt index 3ac906547a..84b4691e12 100644 --- a/vendor/modules.txt +++ b/vendor/modules.txt @@ -301,6 +301,8 @@ github.com/containerd/containerd/services/snapshots github.com/containerd/containerd/services/tasks github.com/containerd/containerd/services/version github.com/containerd/containerd/snapshots +github.com/containerd/containerd/snapshots/devmapper +github.com/containerd/containerd/snapshots/devmapper/dmsetup github.com/containerd/containerd/snapshots/native github.com/containerd/containerd/snapshots/overlay github.com/containerd/containerd/snapshots/proxy The devicemapper pool should be placed to k3s path (e.g DATA_DIR=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.devmapper )","title":"k3s"},{"location":"devmapper/#restart-k8sk3s-services-on-each-node-affected","text":"If you are already running k8s/k3s with a different containerd snapshotter, after configuring devmapper and restarting containerd, you probably need to restart kubelet/k3s services and make sure that all the containers running in k8s.io containerd namespace are now running using devicemapper.","title":"Restart k8s/k3s services on each node affected"},{"location":"examples/","text":"vAccel on Google Coral Edge TPU Running compute-intensive workloads on low-power/Edge devices is a challenging task, especially when multiple factors are being considered, such as security, isolation, performance etc. To this end, one of our initial goals in developing the vAccel framework is to facilitate the acceleration of ML inference operations when executing on low-power devices such as an NVIDIA Jetson Nano. Below we walk through the process of running a simple image classification operation from an Amazon Firecracker VM, using the Maxwell GPU present in the Jetson compute module. Building a vaccel application We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images Running the example Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the Google Coral plugin which implements the image classification operation using libedgetpu and tflite. You can get more info on building this plugin in the relevant section of the currect documentation. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so Finally, the classification application needs the mobilenet model file & labels in the current working path. wget https://github.com/google-coral/test_data/raw/c21de4450f88a20ac5968628d375787745932a5a/mobilenet_v1_1.0_224_quant_edgetpu.tflite wget https://raw.githubusercontent.com/google-coral/test_data/c21de4450f88a20ac5968628d375787745932a5a/imagenet_labels.txt $ ls classify resized_cat.bmp mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt $ ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat Running the same example on an AWS Firecracker VM As mentioned, with vAccel we are able to use the virtio-accel backend while in a VM, to forward specific acceleration operations to the Host OS which, in turn, will execute them on real hardware. Below, we summarize the process to use the Google Coral Edge TPU image classification hardware acceleration functionality from a Firecracker VM. First, we need to have a basic set of binaries to boot a VM and use the virtio-accel backend of vAccelRT. In short, the process to follow is given below: Prepare the system to boot a Firecracker VM Build Firecracker To build the vAccel Firecracker port, we do the following on our aarch64 host: git clone https://github.com/cloudkernels/firecracker -b vaccel-v0.23 cd firecracker/tools/devctr docker build -t nubificus/fcuvm:v0.23 -f Dockerfile.aarch64 cd ../../ tools/devtool build --release -l gnu That should output a binary at build/cargo_target/aarch64-unknown-linux-gnu/release/firecracker relative to the firecracker source tree. Building the kernel & rootfs To facilitate the process of building a kernel image and rootfs for vAccel, you can follow the instructions on the aarch64 tools repo : git clone https://github.com/nubificus/fc-aarch64-guest-build cd fc-aarch64-guest-build bash build.sh If all went well, there should be an output folder with the following files: # ls output Image rootfs.img config_vaccel.json Boot the Firecracker VM Then, using the binaries from the previous steps we spawn a Firecracker VM: export LD_LIBRARY_PATH=${HOME}/.local/lib export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 The output of the above command should be something like the following: ./firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 [ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x410fd034] [ 0.000000] Linux version 4.20.0 (root@buildkitsandbox) (gcc version 8.3.0 (Debian 8.3.0-6)) #1 SMP Thu Dec 3 23:58:32 UTC 2020 [ 0.000000] Machine model: linux,dummy-virt [ 0.000000] earlycon: uart0 at MMIO 0x0000000040003000 (options '') [ 0.000000] printk: bootconsole [uart0] enabled [ 0.000000] efi: Getting EFI parameters from FDT: [ 0.000000] efi: UEFI not found. [ 0.000000] NUMA: No NUMA configuration found [ 0.000000] NUMA: Faking a node at [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] NUMA: NODE_DATA [mem 0x8fedbf00-0x8fef4fff] [ 0.000000] Zone ranges: [ 0.000000] DMA32 [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Normal empty [ 0.000000] Movable zone start for each node [ 0.000000] Early memory node ranges [ 0.000000] node 0: [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Initmem setup node 0 [mem 0x0000000080000000-0x000000008fffffff] [snipped] [ OK ] Reached target Multi-User System. [ OK ] Reached target Graphical Interface. Starting Update UTMP about System Runlevel Changes... [ OK ] Finished Update UTMP about System Runlevel Changes. Ubuntu 20.04.1 LTS localhost.localdomain ttyS0 localhost login: Perform the operation Go ahead and login as root and execute the classification command: localhost login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Sat Feb 6 10:00:59 UTC 2021 on ttyS0 root@localhost:~# ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat root@localhost:~# There you go! Using the Google Coral Edge TPU hardware from a Firecracker VM we were able to perform ML inference!","title":"Examples"},{"location":"examples/#vaccel-on-google-coral-edge-tpu","text":"Running compute-intensive workloads on low-power/Edge devices is a challenging task, especially when multiple factors are being considered, such as security, isolation, performance etc. To this end, one of our initial goals in developing the vAccel framework is to facilitate the acceleration of ML inference operations when executing on low-power devices such as an NVIDIA Jetson Nano. Below we walk through the process of running a simple image classification operation from an Amazon Firecracker VM, using the Maxwell GPU present in the Jetson compute module.","title":"vAccel on Google Coral Edge TPU"},{"location":"examples/#building-a-vaccel-application","text":"We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images","title":"Building a vaccel application"},{"location":"examples/#running-the-example","text":"Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the Google Coral plugin which implements the image classification operation using libedgetpu and tflite. You can get more info on building this plugin in the relevant section of the currect documentation. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so Finally, the classification application needs the mobilenet model file & labels in the current working path. wget https://github.com/google-coral/test_data/raw/c21de4450f88a20ac5968628d375787745932a5a/mobilenet_v1_1.0_224_quant_edgetpu.tflite wget https://raw.githubusercontent.com/google-coral/test_data/c21de4450f88a20ac5968628d375787745932a5a/imagenet_labels.txt $ ls classify resized_cat.bmp mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt $ ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat","title":"Running the example"},{"location":"examples/#running-the-same-example-on-an-aws-firecracker-vm","text":"As mentioned, with vAccel we are able to use the virtio-accel backend while in a VM, to forward specific acceleration operations to the Host OS which, in turn, will execute them on real hardware. Below, we summarize the process to use the Google Coral Edge TPU image classification hardware acceleration functionality from a Firecracker VM. First, we need to have a basic set of binaries to boot a VM and use the virtio-accel backend of vAccelRT. In short, the process to follow is given below:","title":"Running the same example on an AWS Firecracker VM"},{"location":"examples/#prepare-the-system-to-boot-a-firecracker-vm","text":"","title":"Prepare the system to boot a Firecracker VM"},{"location":"examples/#build-firecracker","text":"To build the vAccel Firecracker port, we do the following on our aarch64 host: git clone https://github.com/cloudkernels/firecracker -b vaccel-v0.23 cd firecracker/tools/devctr docker build -t nubificus/fcuvm:v0.23 -f Dockerfile.aarch64 cd ../../ tools/devtool build --release -l gnu That should output a binary at build/cargo_target/aarch64-unknown-linux-gnu/release/firecracker relative to the firecracker source tree.","title":"Build Firecracker"},{"location":"examples/#building-the-kernel-rootfs","text":"To facilitate the process of building a kernel image and rootfs for vAccel, you can follow the instructions on the aarch64 tools repo : git clone https://github.com/nubificus/fc-aarch64-guest-build cd fc-aarch64-guest-build bash build.sh If all went well, there should be an output folder with the following files: # ls output Image rootfs.img config_vaccel.json","title":"Building the kernel &amp; rootfs"},{"location":"examples/#boot-the-firecracker-vm","text":"Then, using the binaries from the previous steps we spawn a Firecracker VM: export LD_LIBRARY_PATH=${HOME}/.local/lib export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 The output of the above command should be something like the following: ./firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 [ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x410fd034] [ 0.000000] Linux version 4.20.0 (root@buildkitsandbox) (gcc version 8.3.0 (Debian 8.3.0-6)) #1 SMP Thu Dec 3 23:58:32 UTC 2020 [ 0.000000] Machine model: linux,dummy-virt [ 0.000000] earlycon: uart0 at MMIO 0x0000000040003000 (options '') [ 0.000000] printk: bootconsole [uart0] enabled [ 0.000000] efi: Getting EFI parameters from FDT: [ 0.000000] efi: UEFI not found. [ 0.000000] NUMA: No NUMA configuration found [ 0.000000] NUMA: Faking a node at [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] NUMA: NODE_DATA [mem 0x8fedbf00-0x8fef4fff] [ 0.000000] Zone ranges: [ 0.000000] DMA32 [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Normal empty [ 0.000000] Movable zone start for each node [ 0.000000] Early memory node ranges [ 0.000000] node 0: [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Initmem setup node 0 [mem 0x0000000080000000-0x000000008fffffff] [snipped] [ OK ] Reached target Multi-User System. [ OK ] Reached target Graphical Interface. Starting Update UTMP about System Runlevel Changes... [ OK ] Finished Update UTMP about System Runlevel Changes. Ubuntu 20.04.1 LTS localhost.localdomain ttyS0 localhost login:","title":"Boot the Firecracker VM"},{"location":"examples/#perform-the-operation","text":"Go ahead and login as root and execute the classification command: localhost login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Sat Feb 6 10:00:59 UTC 2021 on ttyS0 root@localhost:~# ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat root@localhost:~# There you go! Using the Google Coral Edge TPU hardware from a Firecracker VM we were able to perform ML inference!","title":"Perform the operation"},{"location":"jetson/","text":"Jetson-inference vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload with the jetson-inference plugin, we will use a set of NVIDIA GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. The Jetson-inference vAccel plugin is based on jetson-inference , a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. Setup NVIDIA custom repos The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Install CUDA and TensorRT Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Prepare for building jetson-inference Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference Patch to support GTX 1030 & RTX 2060 To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda Building jetson-inference We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install","title":"Jetson-inference"},{"location":"jetson/#jetson-inference","text":"vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload with the jetson-inference plugin, we will use a set of NVIDIA GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. The Jetson-inference vAccel plugin is based on jetson-inference , a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system.","title":"Jetson-inference"},{"location":"jetson/#setup-nvidia-custom-repos","text":"The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update","title":"Setup NVIDIA custom repos"},{"location":"jetson/#install-cuda-and-tensorrt","text":"Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7","title":"Install CUDA and TensorRT"},{"location":"jetson/#prepare-for-building-jetson-inference","text":"Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference","title":"Prepare for building jetson-inference"},{"location":"jetson/#patch-to-support-gtx-1030-rtx-2060","text":"To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda","title":"Patch to support GTX 1030 &amp; RTX 2060"},{"location":"jetson/#building-jetson-inference","text":"We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install","title":"Building jetson-inference"},{"location":"kata-vaccel/","text":"vAccel on kata containers We have ported vAccel to the Kata Containers Runtime for Firecracker with containerd Shim v2. You can try it directly on Kubernetes following the docs ! Visit our fork repos to have a look on our WiP to fully integrate vAccel with Kata Containers on QEMU / Firecracker. vaccel-kata-runtime vaccel-kata-agent","title":"vAccel on kata containers"},{"location":"kata-vaccel/#vaccel-on-kata-containers","text":"We have ported vAccel to the Kata Containers Runtime for Firecracker with containerd Shim v2. You can try it directly on Kubernetes following the docs ! Visit our fork repos to have a look on our WiP to fully integrate vAccel with Kata Containers on QEMU / Firecracker. vaccel-kata-runtime vaccel-kata-agent","title":"vAccel on kata containers"},{"location":"plugin/","text":"Add a plugin To implement a vAccel plugin, all we need to do is add the implementation of the relevant frontend function and register it to the vAccel plugin subsystem. To better understand the steps, it would be useful to go through lab1 . Get the code First, lets grab the code: git clone --recursive https://github.com/cloudkernels/vaccelrt cd vaccelrt Clone the noop plugin We will use the noop plugin as a skeleton for our new plugin which we will call helloworld . For simplicity, we copy the noop plugin directory structure to a new directory: cp -avf plugins/noop plugins/helloworld we replace noop with the name of our choosing ( helloworld ) so we come up with the following directory structure: $ tree -L 1 plugins/helloworld plugins/helloworld \u251c\u2500\u2500 CMakeLists.txt \u2514\u2500\u2500 vaccel.c We need to change the contents of plugins/helloworld/CMakeLists.txt to reflect the noop to helloworld change: set(include_dirs ${CMAKE_SOURCE_DIR}/src) set(SOURCES vaccel.c ${include_dirs}/vaccel.h ${include_dirs}/plugin.h) add_library(vaccel-helloworld SHARED ${SOURCES}) target_include_directories(vaccel-helloworld PRIVATE ${include_dirs}) # Setup make install install(TARGETS vaccel-helloworld DESTINATION \"${lib_path}\") Similarly, we replace the plugin implementation with our own in vaccel.c , adding a simple message: #include <stdio.h> #include <plugin.h> static int helloworld(struct vaccel_session *session) { fprintf(stdout, \"Calling vaccel-helloworld for session %u\\n\", session->session_id); printf(\"_______________________________________________________________\\n\\n\"); printf(\"This is the helloworld plugin, implementing the NOOP operation!\\n\"); printf(\"===============================================================\\n\\n\"); return VACCEL_OK; } struct vaccel_op op = VACCEL_OP_INIT(op, VACCEL_NO_OP, helloworld); static int init(void) { return register_plugin_function(&op); } static int fini(void) { return VACCEL_OK; } VACCEL_MODULE( .name = \"helloworld\", .version = \"0.1\", .init = init, .fini = fini ) Additionally, we need to add the relevant rules in the CMakeLists.txt files to build the plugin along with the rest of the vAccelRT code. We add the following to plugins/CMakeLists.txt : if (BUILD_PLUGIN_HELLOWORLD) add_subdirectory(helloworld) endif(BUILD_PLUGIN_HELLOWORLD) and to CMakeLists.txt : option(BUILD_PLUGIN_HELLOWORLD \"Build the hello-world debugging plugin\" OFF) Now we can build our new plugin, by specifying the new option we just added: Build our helloworld plugin cd build cmake ../ -DBUILD_PLUGIN_HELLOWORLD=ON make Use our helloworld plugin We see that a new plugin is now available, libvaccel-helloworld.so . Lets use this one instead of the noop one! $ LD_LIBRARY_PATH=src VACCEL_DEBUG_LEVEL=4 VACCEL_BACKENDS=./plugins/helloworld/libvaccel-helloworld.so ./hello_world 2021.04.09-12:40:45.86 - <debug> Initializing vAccel 2021.04.09-12:40:45.86 - <debug> Registered plugin helloworld 2021.04.09-12:40:45.86 - <debug> Registered function noop from plugin helloworld 2021.04.09-12:40:45.86 - <debug> Loaded plugin helloworld from ./plugins/helloworld/libvaccel-helloworld.so 2021.04.09-12:40:45.86 - <debug> session:1 New session Initialized session with id: 1 2021.04.09-12:40:45.86 - <debug> session:1 Looking for plugin implementing noop 2021.04.09-12:40:45.86 - <debug> Found implementation in helloworld plugin Calling vaccel-helloworld for session 1 _______________________________________________________________ This is the helloworld plugin, implementing the NOOP operation! =============================================================== 2021.04.09-12:40:45.86 - <debug> session:1 Free session 2021.04.09-12:40:45.86 - <debug> Shutting down vAccel 2021.04.09-12:40:45.86 - <debug> Cleaning up plugins 2021.04.09-12:40:45.86 - <debug> Unregistered plugin helloworld Takeaway We made it! We just added our own plugin to implement a vAccel operation: - we integrated it into the vAccel source tree, - we built it and - used it to run the noop operation.","title":"Add a plugin"},{"location":"plugin/#add-a-plugin","text":"To implement a vAccel plugin, all we need to do is add the implementation of the relevant frontend function and register it to the vAccel plugin subsystem. To better understand the steps, it would be useful to go through lab1 .","title":"Add a plugin"},{"location":"plugin/#get-the-code","text":"First, lets grab the code: git clone --recursive https://github.com/cloudkernels/vaccelrt cd vaccelrt","title":"Get the code"},{"location":"plugin/#clone-the-noop-plugin","text":"We will use the noop plugin as a skeleton for our new plugin which we will call helloworld . For simplicity, we copy the noop plugin directory structure to a new directory: cp -avf plugins/noop plugins/helloworld we replace noop with the name of our choosing ( helloworld ) so we come up with the following directory structure: $ tree -L 1 plugins/helloworld plugins/helloworld \u251c\u2500\u2500 CMakeLists.txt \u2514\u2500\u2500 vaccel.c We need to change the contents of plugins/helloworld/CMakeLists.txt to reflect the noop to helloworld change: set(include_dirs ${CMAKE_SOURCE_DIR}/src) set(SOURCES vaccel.c ${include_dirs}/vaccel.h ${include_dirs}/plugin.h) add_library(vaccel-helloworld SHARED ${SOURCES}) target_include_directories(vaccel-helloworld PRIVATE ${include_dirs}) # Setup make install install(TARGETS vaccel-helloworld DESTINATION \"${lib_path}\") Similarly, we replace the plugin implementation with our own in vaccel.c , adding a simple message: #include <stdio.h> #include <plugin.h> static int helloworld(struct vaccel_session *session) { fprintf(stdout, \"Calling vaccel-helloworld for session %u\\n\", session->session_id); printf(\"_______________________________________________________________\\n\\n\"); printf(\"This is the helloworld plugin, implementing the NOOP operation!\\n\"); printf(\"===============================================================\\n\\n\"); return VACCEL_OK; } struct vaccel_op op = VACCEL_OP_INIT(op, VACCEL_NO_OP, helloworld); static int init(void) { return register_plugin_function(&op); } static int fini(void) { return VACCEL_OK; } VACCEL_MODULE( .name = \"helloworld\", .version = \"0.1\", .init = init, .fini = fini ) Additionally, we need to add the relevant rules in the CMakeLists.txt files to build the plugin along with the rest of the vAccelRT code. We add the following to plugins/CMakeLists.txt : if (BUILD_PLUGIN_HELLOWORLD) add_subdirectory(helloworld) endif(BUILD_PLUGIN_HELLOWORLD) and to CMakeLists.txt : option(BUILD_PLUGIN_HELLOWORLD \"Build the hello-world debugging plugin\" OFF) Now we can build our new plugin, by specifying the new option we just added:","title":"Clone the noop plugin"},{"location":"plugin/#build-our-helloworld-plugin","text":"cd build cmake ../ -DBUILD_PLUGIN_HELLOWORLD=ON make","title":"Build our helloworld plugin"},{"location":"plugin/#use-our-helloworld-plugin","text":"We see that a new plugin is now available, libvaccel-helloworld.so . Lets use this one instead of the noop one! $ LD_LIBRARY_PATH=src VACCEL_DEBUG_LEVEL=4 VACCEL_BACKENDS=./plugins/helloworld/libvaccel-helloworld.so ./hello_world 2021.04.09-12:40:45.86 - <debug> Initializing vAccel 2021.04.09-12:40:45.86 - <debug> Registered plugin helloworld 2021.04.09-12:40:45.86 - <debug> Registered function noop from plugin helloworld 2021.04.09-12:40:45.86 - <debug> Loaded plugin helloworld from ./plugins/helloworld/libvaccel-helloworld.so 2021.04.09-12:40:45.86 - <debug> session:1 New session Initialized session with id: 1 2021.04.09-12:40:45.86 - <debug> session:1 Looking for plugin implementing noop 2021.04.09-12:40:45.86 - <debug> Found implementation in helloworld plugin Calling vaccel-helloworld for session 1 _______________________________________________________________ This is the helloworld plugin, implementing the NOOP operation! =============================================================== 2021.04.09-12:40:45.86 - <debug> session:1 Free session 2021.04.09-12:40:45.86 - <debug> Shutting down vAccel 2021.04.09-12:40:45.86 - <debug> Cleaning up plugins 2021.04.09-12:40:45.86 - <debug> Unregistered plugin helloworld","title":"Use our helloworld plugin"},{"location":"plugin/#takeaway","text":"We made it! We just added our own plugin to implement a vAccel operation: - we integrated it into the vAccel source tree, - we built it and - used it to run the noop operation.","title":"Takeaway"},{"location":"quickstart/","text":"Quickstart This is a quick start guide for running a simple vAccel application on a x86 machine with an Nvidia GPU. We will build the vAccel stack and run a simple application both directly on a Linux host and inside a Firecracker VM. Building the vAccel stack The vAccel repo serves as a means to keep track of all the individual components needed to run a vAccel application. git clone https://github.com/cloudkernels/vaccel cd vaccel The repo includes a build script which will build all the components: The vAccelRT with the currently supported plugins . The virtio-accel kernel module which acts as the transport layer when running inside the Firecracker VM The forked Firecracker VMM which is patched to handle virtio-accel requests A rootfs and suitable vmlinux kernel image for booting the Firecracker VM An example application for testing our setup Building the Jetson inference plugin requires a working jetson-inference installation along with the corresponding CUDA environment on the machine. If you have it you can just run: ./build.sh all Note: while building the rootfs, this script will require your sudo password. If a jetson-inference setup is not available you can either follow this guide to build the vAccel jetson plugin and install the prerequisites on your host machine, or use our nubificus/vaccel-deps container image to build the necessary components, by running: ./build.sh -c all Running the application The above build process create two directories: build includes intermediate files of the building process and output includes the build artifacts we will need to run our example: cd output/debug ls bin include lib share Native execution Running on the host In case you have the jetson-inference framework installed on your host, we simply can execute: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Run the image classification example ./bin/classify ./share/images/dog_0.jpg 1 You should some logging from the jetson-inference framework and in the end of it you should get the classification tags of the image: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Note : The first time your run a classification with a model jetson-inference is performing some JIT steps to optimize the classification result, so you can expect increased execution time. The output of this operation is cached for subsequent executions. Running in a Firecracker VM build.sh created for us a rootfs image and a vmlinux kernel for booting a Firecracker VM. The rootfs has pre-installed the vAccel libraries, the example application the images and models. Additionally, it installed for us the following Firecracker configuration for booting the VM. In similar fashion as before, we launch the VM: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Launch the Firecracker VM ./bin/firecracker --api-sock fc.sock --config-file share/config_virtio_accel.json --seccomp-level 0 This will launch the Firecracker VM and it will result in a login prompt in the VM, to which we can login using the root user without a password. Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0 vaccel-guest login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Fri Feb 5 17:32:12 UTC 2021 on ttyS0 root@vaccel-guest:~# Once, in the prompt of the guest we setup the environment and run the example: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1 Note : In this case, we use the VirtIO plugin since we are inside the VM and we do not need to define the path to the pre-trained model. The latter is necessary only on the host where we use the jetson plugin. The result of the classification should be the same as before: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Docker execution If you don't have a Jetson-inference installation available on your machine, you can use our nubificus/vaccel-deps Docker image to run a vAccel application. You need to install the Nvidia container runtime following the instructions here , which allows you to use NVIDIA GPU devices inside Docker containers. Once, you're all setup with the nvidia runtime you can run the application: docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/classify share/images/dog_0.jpg 1 Similarly, we can launch a Firecracker VM inside the container. In this case, we need to add the --privileged flag to allow launching VMs inside the container and -it to be able to use the VM's console. docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ --privileged \\ -it \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/firecracker --api-sock fc.sock --config-file ./share/config_virtio_accel.json --seccomp-level 0 which we'll give us a console inside the VM, from which we can run our application the same way as we did before: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This is a quick start guide for running a simple vAccel application on a x86 machine with an Nvidia GPU. We will build the vAccel stack and run a simple application both directly on a Linux host and inside a Firecracker VM.","title":"Quickstart"},{"location":"quickstart/#building-the-vaccel-stack","text":"The vAccel repo serves as a means to keep track of all the individual components needed to run a vAccel application. git clone https://github.com/cloudkernels/vaccel cd vaccel The repo includes a build script which will build all the components: The vAccelRT with the currently supported plugins . The virtio-accel kernel module which acts as the transport layer when running inside the Firecracker VM The forked Firecracker VMM which is patched to handle virtio-accel requests A rootfs and suitable vmlinux kernel image for booting the Firecracker VM An example application for testing our setup Building the Jetson inference plugin requires a working jetson-inference installation along with the corresponding CUDA environment on the machine. If you have it you can just run: ./build.sh all Note: while building the rootfs, this script will require your sudo password. If a jetson-inference setup is not available you can either follow this guide to build the vAccel jetson plugin and install the prerequisites on your host machine, or use our nubificus/vaccel-deps container image to build the necessary components, by running: ./build.sh -c all","title":"Building the vAccel stack"},{"location":"quickstart/#running-the-application","text":"The above build process create two directories: build includes intermediate files of the building process and output includes the build artifacts we will need to run our example: cd output/debug ls bin include lib share","title":"Running the application"},{"location":"quickstart/#native-execution","text":"","title":"Native execution"},{"location":"quickstart/#running-on-the-host","text":"In case you have the jetson-inference framework installed on your host, we simply can execute: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Run the image classification example ./bin/classify ./share/images/dog_0.jpg 1 You should some logging from the jetson-inference framework and in the end of it you should get the classification tags of the image: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Note : The first time your run a classification with a model jetson-inference is performing some JIT steps to optimize the classification result, so you can expect increased execution time. The output of this operation is cached for subsequent executions.","title":"Running on the host"},{"location":"quickstart/#running-in-a-firecracker-vm","text":"build.sh created for us a rootfs image and a vmlinux kernel for booting a Firecracker VM. The rootfs has pre-installed the vAccel libraries, the example application the images and models. Additionally, it installed for us the following Firecracker configuration for booting the VM. In similar fashion as before, we launch the VM: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Launch the Firecracker VM ./bin/firecracker --api-sock fc.sock --config-file share/config_virtio_accel.json --seccomp-level 0 This will launch the Firecracker VM and it will result in a login prompt in the VM, to which we can login using the root user without a password. Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0 vaccel-guest login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Fri Feb 5 17:32:12 UTC 2021 on ttyS0 root@vaccel-guest:~# Once, in the prompt of the guest we setup the environment and run the example: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1 Note : In this case, we use the VirtIO plugin since we are inside the VM and we do not need to define the path to the pre-trained model. The latter is necessary only on the host where we use the jetson plugin. The result of the classification should be the same as before: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute","title":"Running in a Firecracker VM"},{"location":"quickstart/#docker-execution","text":"If you don't have a Jetson-inference installation available on your machine, you can use our nubificus/vaccel-deps Docker image to run a vAccel application. You need to install the Nvidia container runtime following the instructions here , which allows you to use NVIDIA GPU devices inside Docker containers. Once, you're all setup with the nvidia runtime you can run the application: docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/classify share/images/dog_0.jpg 1 Similarly, we can launch a Firecracker VM inside the container. In this case, we need to add the --privileged flag to allow launching VMs inside the container and -it to be able to use the VM's console. docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ --privileged \\ -it \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/firecracker --api-sock fc.sock --config-file ./share/config_virtio_accel.json --seccomp-level 0 which we'll give us a console inside the VM, from which we can run our application the same way as we did before: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1","title":"Docker execution"},{"location":"run/","text":"Running a simple example Building a vaccel application We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: mkdir build cd build cmake -DBUILD_EXAMPLES=ON .. make ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : cd ../examples gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl ls classification.c classify CMakeLists.txt images Running the example Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: ls classify images networks VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running a simple example"},{"location":"run/#running-a-simple-example","text":"","title":"Running a simple example"},{"location":"run/#building-a-vaccel-application","text":"We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: mkdir build cd build cmake -DBUILD_EXAMPLES=ON .. make ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : cd ../examples gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl ls classification.c classify CMakeLists.txt images","title":"Building a vaccel application"},{"location":"run/#running-the-example","text":"Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: ls classify images networks VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running the example"},{"location":"usecases/","text":"Use Cases HW Acceleration on Serverless deployments Serverless Computing has gained significant ground over the past couple of years, especially due to its unprecedented \"on-demand\" computing offering at a fraction of the price of having a dedicated VM or bare-metal machine. However, apart from the limitation that one has to re-write its application to match the Serverless computing paradigm (micro-service, event triggered), access to \"specialized\" equipement is not something common / supported now. Major serverless offerings such as AWS Lambda, Azure functions, or Google Functions do not offer access to hardware acceleration for even the most common tasks (ML inference). To work around this issue, and study the implications of hardware acceleration in serverless computing we started experimenting on OpenFaaS and its integration with vAccel Given OpenFaaS huge community support, installing it on a working k8s cluster is a piece of cake. You can find more information in the following document: Deployment guide for Kubernetes Assuming you have a working OpenFaaS installation, leveraging the hardware acceleration capabilities of your cluster with vAccel should be really straightforward. Create a new OpenFaaS Profile To take advantage of vAccel on Firecracker via OpenFaaS, all you have to do, is tell OpenFaaS to spawn the relevant functions using a different profile. To allow maximum flexibility without overloading the OpenFaaS function configuration, the OpenFaaS developers have introduced the concept of Profiles. This is simply a reserved function annotation that the faas-provider can detect and use to apply the advanced configuration. Profiles must be pre-created, similar to Secrets, by the cluster admin. When installing OpenFaaS on Kubernetes, Profiles use a CRD. This must be installed during or prior to start the OpenFaaS controller. When using the official Helm chart this will happen automatically. Alternatively, you can apply this YAML to install the CRD. So, let's create a hardware acceleration profile on our OpenFaaS installation! As mentioned earlier in the docs, to deploy vAccel-able workloads on a k8s cluster we have to follow the relevant instructions . Essentially, we leverage the awesome work done by the kata-containers team and provide an alternative to the generic Firecracker RuntimeClass , kata-fc. So, assuming you have a working cluster with vAccel installed on k8s , all you need to do to run an OpenFaaS function there is to create a Profile: kubectl apply -f- << EOF kind: Profile apiVersion: openfaas.com/v1 metadata: name: gvisor namespace: openfaas spec: runtimeClassName: kata-fc EOF Build an image classification function example Then, we need to build a function that consumes the vAccelRT API, as exposed to the available functions. Check out this repo to get an idea of how easy it is to create a function that classifies images ;-) git clone https://github.com/nubificus/stdinout -b vaccel cd stdinout Examine test.c : it is a simple file that gets its input from stdin, stores it into a memory buffer and pushes it to be classified, using the vAccel API. The output is a string of the classification label, along with the accuracy given by the specific model used. Lets build this simple function: make The output of this command is an executable that given an image from STDIN will output the classification text, consuming the vAccel API. More or less, something like the following: # Set the library path to ./ for libfileread.so and # /usr/local/lib for libvaccel.so export LD_LIBRARY_PATH=./:/usr/local/lib # set the vAccel Backend (running on an RTX 2060 Super export VACCEL_BACKENDS=/usr/local/lib/libvaccel-jetson.so cat dog_0.jpg | ./test Given the input is the file below: the output should be something like the following: Initialized session with id: 1 imagenet: 60.54688% class #249 (malamute, malemute, Alaskan malamute) classification tags: 60.547% malamute, malemute, Alaskan malamute Integrate this operation into a function for OpenFaaS To integrate this into a function, we first need to build a container image, with the libvaccel library along with the vAccel-virtio backend (Since this workload is going to be running on a Firecracker VM ;-). docker build -t nubificus/fclassify -f Dockerfile.virtio . Then, a simple YAML like the following ( fclassify.yaml ) will instantiate a function that will execute the fprocess: directive, in our case, test : provider: name: openfaas gateway: http://myclusternode:31112 functions: fclassify: skip_build: false image: nubificus/fclassify fprocess: \"/test\" annotations: com.openfaas.profile: kata labels: com.openfaas.scale.min: 12 com.openfaas.scale.max: 100 com.openfaas.scale.factor: 40 limits: cpu: 10m requests: cpu: 10m The important parts to note at this YAML file are: com.openfaas.profile: kata which makes sure that the function will be invoked on the kata profile, using the kata-fc RuntimeClass (which supports vAccel). and image: nubificus/fclassify fprocess: \"/test\" which denotes that the image to be used is nubificus/fclassify, built earlier using Dockerfile.virtio , with /test being the entrypoint. Deploy the function Finally, using the command below: faas-cli deploy -f fclassify.yaml will instatiate the number of replicas chosen for fclassify as well as listen to requests for image classification, ready to forward them to the actual hardware securely and efficiently! Consume the function Given an ingress rule that will route traffic to the fclassify function, we can invoke this operation externally. For instance, given the following three images: we can invoke the OpenFaaS function on our local K8s OpenFaaS cluster: for x in object_0.jpg object_2.jpg object_3.jpg do cat $x | curl -L -X POST https://openfaas.nbfc.io/function/fclassify \\ --data-binary \"@$x\" & done and the output should be: [1] 3129602 [2] 3129604 [3] 3129606 Initialized session with id: 1 classification tags: 23.145% mountain bike, all-terrain bike, off-roader Initialized session with id: 1 classification tags: 45.776% freight car Initialized session with id: 1 classification tags: 90.771% canoe","title":"Use Cases"},{"location":"usecases/#use-cases","text":"","title":"Use Cases"},{"location":"usecases/#hw-acceleration-on-serverless-deployments","text":"Serverless Computing has gained significant ground over the past couple of years, especially due to its unprecedented \"on-demand\" computing offering at a fraction of the price of having a dedicated VM or bare-metal machine. However, apart from the limitation that one has to re-write its application to match the Serverless computing paradigm (micro-service, event triggered), access to \"specialized\" equipement is not something common / supported now. Major serverless offerings such as AWS Lambda, Azure functions, or Google Functions do not offer access to hardware acceleration for even the most common tasks (ML inference). To work around this issue, and study the implications of hardware acceleration in serverless computing we started experimenting on OpenFaaS and its integration with vAccel Given OpenFaaS huge community support, installing it on a working k8s cluster is a piece of cake. You can find more information in the following document: Deployment guide for Kubernetes Assuming you have a working OpenFaaS installation, leveraging the hardware acceleration capabilities of your cluster with vAccel should be really straightforward.","title":"HW Acceleration on Serverless deployments"},{"location":"usecases/#create-a-new-openfaas-profile","text":"To take advantage of vAccel on Firecracker via OpenFaaS, all you have to do, is tell OpenFaaS to spawn the relevant functions using a different profile. To allow maximum flexibility without overloading the OpenFaaS function configuration, the OpenFaaS developers have introduced the concept of Profiles. This is simply a reserved function annotation that the faas-provider can detect and use to apply the advanced configuration. Profiles must be pre-created, similar to Secrets, by the cluster admin. When installing OpenFaaS on Kubernetes, Profiles use a CRD. This must be installed during or prior to start the OpenFaaS controller. When using the official Helm chart this will happen automatically. Alternatively, you can apply this YAML to install the CRD. So, let's create a hardware acceleration profile on our OpenFaaS installation! As mentioned earlier in the docs, to deploy vAccel-able workloads on a k8s cluster we have to follow the relevant instructions . Essentially, we leverage the awesome work done by the kata-containers team and provide an alternative to the generic Firecracker RuntimeClass , kata-fc. So, assuming you have a working cluster with vAccel installed on k8s , all you need to do to run an OpenFaaS function there is to create a Profile: kubectl apply -f- << EOF kind: Profile apiVersion: openfaas.com/v1 metadata: name: gvisor namespace: openfaas spec: runtimeClassName: kata-fc EOF","title":"Create a new OpenFaaS Profile"},{"location":"usecases/#build-an-image-classification-function-example","text":"Then, we need to build a function that consumes the vAccelRT API, as exposed to the available functions. Check out this repo to get an idea of how easy it is to create a function that classifies images ;-) git clone https://github.com/nubificus/stdinout -b vaccel cd stdinout Examine test.c : it is a simple file that gets its input from stdin, stores it into a memory buffer and pushes it to be classified, using the vAccel API. The output is a string of the classification label, along with the accuracy given by the specific model used. Lets build this simple function: make The output of this command is an executable that given an image from STDIN will output the classification text, consuming the vAccel API. More or less, something like the following: # Set the library path to ./ for libfileread.so and # /usr/local/lib for libvaccel.so export LD_LIBRARY_PATH=./:/usr/local/lib # set the vAccel Backend (running on an RTX 2060 Super export VACCEL_BACKENDS=/usr/local/lib/libvaccel-jetson.so cat dog_0.jpg | ./test Given the input is the file below: the output should be something like the following: Initialized session with id: 1 imagenet: 60.54688% class #249 (malamute, malemute, Alaskan malamute) classification tags: 60.547% malamute, malemute, Alaskan malamute","title":"Build an image classification function example"},{"location":"usecases/#integrate-this-operation-into-a-function-for-openfaas","text":"To integrate this into a function, we first need to build a container image, with the libvaccel library along with the vAccel-virtio backend (Since this workload is going to be running on a Firecracker VM ;-). docker build -t nubificus/fclassify -f Dockerfile.virtio . Then, a simple YAML like the following ( fclassify.yaml ) will instantiate a function that will execute the fprocess: directive, in our case, test : provider: name: openfaas gateway: http://myclusternode:31112 functions: fclassify: skip_build: false image: nubificus/fclassify fprocess: \"/test\" annotations: com.openfaas.profile: kata labels: com.openfaas.scale.min: 12 com.openfaas.scale.max: 100 com.openfaas.scale.factor: 40 limits: cpu: 10m requests: cpu: 10m The important parts to note at this YAML file are: com.openfaas.profile: kata which makes sure that the function will be invoked on the kata profile, using the kata-fc RuntimeClass (which supports vAccel). and image: nubificus/fclassify fprocess: \"/test\" which denotes that the image to be used is nubificus/fclassify, built earlier using Dockerfile.virtio , with /test being the entrypoint.","title":"Integrate this operation into a function for OpenFaaS"},{"location":"usecases/#deploy-the-function","text":"Finally, using the command below: faas-cli deploy -f fclassify.yaml will instatiate the number of replicas chosen for fclassify as well as listen to requests for image classification, ready to forward them to the actual hardware securely and efficiently!","title":"Deploy the function"},{"location":"usecases/#consume-the-function","text":"Given an ingress rule that will route traffic to the fclassify function, we can invoke this operation externally. For instance, given the following three images: we can invoke the OpenFaaS function on our local K8s OpenFaaS cluster: for x in object_0.jpg object_2.jpg object_3.jpg do cat $x | curl -L -X POST https://openfaas.nbfc.io/function/fclassify \\ --data-binary \"@$x\" & done and the output should be: [1] 3129602 [2] 3129604 [3] 3129606 Initialized session with id: 1 classification tags: 23.145% mountain bike, all-terrain bike, off-roader Initialized session with id: 1 classification tags: 45.776% freight car Initialized session with id: 1 classification tags: 90.771% canoe","title":"Consume the function"},{"location":"vaccelrt/","text":"Building the vAccel Runtime vAccelRT vAccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework. Build & Installation 1. Cloning and preparing the build directory ~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb 2. Building the core runtime system # This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install 3. Building the plugins Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin. VirtIO plugin building options Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable. Jetson plugin building options The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Building the vAccel Runtime"},{"location":"vaccelrt/#building-the-vaccel-runtime","text":"","title":"Building the vAccel Runtime"},{"location":"vaccelrt/#vaccelrt","text":"vAccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework.","title":"vAccelRT"},{"location":"vaccelrt/#build-installation","text":"","title":"Build &amp; Installation"},{"location":"vaccelrt/#1-cloning-and-preparing-the-build-directory","text":"~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb","title":"1. Cloning and preparing the build directory"},{"location":"vaccelrt/#2-building-the-core-runtime-system","text":"# This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install","title":"2. Building the core runtime system"},{"location":"vaccelrt/#3-building-the-plugins","text":"Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin.","title":"3. Building the plugins"},{"location":"vaccelrt/#virtio-plugin-building-options","text":"Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable.","title":"VirtIO plugin building options"},{"location":"vaccelrt/#jetson-plugin-building-options","text":"The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Jetson plugin building options"},{"location":"virtio/","text":"Virtio Accel Virtio Accel was inspired by the Virtio Crypto paravirtual device for enabling accelerated crypto operations inside a Virtual Machine. We extended the idea of accelerated crypto operations to any acceleratable operation, which resulted in the virtio-accel module and the corresponding vAccelRT plugin that uses it to allow vAccel applications to use accelerated operations from within a Virtual Machine. Currently we support the AWS Firecracker and QEMU hypervisors. Building the kernel module We first need to build the virtio-accel module that will be used inside our VM. We will need the source of the kernel module and a kernel tree. Firecracker For Firecracker we use Linux kernel version 4.20 , because that is consistent with the configuration the AWS Firecracker team is shipping but the module itself should be able to build with newer kernel versions, as well. For example, for our k8s deployment of vAccel we use Linux Kernel version 5.4.60 Let's fetch and build the kernel # Fetch the Linux kernel tree git clone --depth=1 -b v4.20 https://github.com/torvalds/linux.git cd linux # Fetch the Firecracker config wget https://raw.githubusercontent.com/firecracker-microvm/firecracker/master/resources/microvm-kernel-x86_64.config -O arch/x86/configs/microvm.config touch .config make microvm.config make vmlinux cd .. ls linux/vmlinux linux/vmlinux You should now have a newly built kernel image under linux/vmlinux , which you can use to boot with your Firecracker VM. And now we can build the module: git clone https://github.com/cloudkernels/virtio-accel make -C virtio-accel KDIR=$(pwd)/linux ZC=0 ls virtio-accel/virtio_accel.ko virtio-accel/virtio_accel.ko This should build the module under virtio-accel/virtio_accel.ko . Note : The virtio-accel module supports zero-copy operations. This functionality has not been implemented yet in Firecracker, so we disabled the feature passing ZC=0 to the Makefile . QEMU TBD Building the vAccelRT plugin The vAccelRT runtime ships with a virtio plugin which speaks the virtio-accel module's ioctl language to offload computation from a VM guest to the host. # Fetch the vAccelRT repo git clone --recursive https://github.com/cloudkernels/vaccelrt cd vaccelrt mkdir build cd build cmake .. -DBUILD_PLUGIN_VIRTIO=ON -DVIRTIO_ACCEL_ROOT=../../virtio-accel make cd ../../ ls vaccelrt/build/plugins/virtio/libvaccel-virtio.so vaccelrt/build/plugins/virtio/libvaccel-virtio.so This builds the plugin and places it under vaccelrt/build/plugins/virtio/libvaccel-virtio.so . You can now insert the plugin to your VMs rootfs and run a vAccel application.","title":"Virtio Accel"},{"location":"virtio/#virtio-accel","text":"Virtio Accel was inspired by the Virtio Crypto paravirtual device for enabling accelerated crypto operations inside a Virtual Machine. We extended the idea of accelerated crypto operations to any acceleratable operation, which resulted in the virtio-accel module and the corresponding vAccelRT plugin that uses it to allow vAccel applications to use accelerated operations from within a Virtual Machine. Currently we support the AWS Firecracker and QEMU hypervisors.","title":"Virtio Accel"},{"location":"virtio/#building-the-kernel-module","text":"We first need to build the virtio-accel module that will be used inside our VM. We will need the source of the kernel module and a kernel tree.","title":"Building the kernel module"},{"location":"virtio/#firecracker","text":"For Firecracker we use Linux kernel version 4.20 , because that is consistent with the configuration the AWS Firecracker team is shipping but the module itself should be able to build with newer kernel versions, as well. For example, for our k8s deployment of vAccel we use Linux Kernel version 5.4.60 Let's fetch and build the kernel # Fetch the Linux kernel tree git clone --depth=1 -b v4.20 https://github.com/torvalds/linux.git cd linux # Fetch the Firecracker config wget https://raw.githubusercontent.com/firecracker-microvm/firecracker/master/resources/microvm-kernel-x86_64.config -O arch/x86/configs/microvm.config touch .config make microvm.config make vmlinux cd .. ls linux/vmlinux linux/vmlinux You should now have a newly built kernel image under linux/vmlinux , which you can use to boot with your Firecracker VM. And now we can build the module: git clone https://github.com/cloudkernels/virtio-accel make -C virtio-accel KDIR=$(pwd)/linux ZC=0 ls virtio-accel/virtio_accel.ko virtio-accel/virtio_accel.ko This should build the module under virtio-accel/virtio_accel.ko . Note : The virtio-accel module supports zero-copy operations. This functionality has not been implemented yet in Firecracker, so we disabled the feature passing ZC=0 to the Makefile .","title":"Firecracker"},{"location":"virtio/#qemu","text":"TBD","title":"QEMU"},{"location":"virtio/#building-the-vaccelrt-plugin","text":"The vAccelRT runtime ships with a virtio plugin which speaks the virtio-accel module's ioctl language to offload computation from a VM guest to the host. # Fetch the vAccelRT repo git clone --recursive https://github.com/cloudkernels/vaccelrt cd vaccelrt mkdir build cd build cmake .. -DBUILD_PLUGIN_VIRTIO=ON -DVIRTIO_ACCEL_ROOT=../../virtio-accel make cd ../../ ls vaccelrt/build/plugins/virtio/libvaccel-virtio.so vaccelrt/build/plugins/virtio/libvaccel-virtio.so This builds the plugin and places it under vaccelrt/build/plugins/virtio/libvaccel-virtio.so . You can now insert the plugin to your VMs rootfs and run a vAccel application.","title":"Building the vAccelRT plugin"},{"location":"k8s/kata/","text":"vAccel on k8s using Kata-containers & Firecracker Prerequisites In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter ( info ) nvidia GPU which supports CUDA ( for now ) ( info ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries) ( info ) Quick start Deploy vAccel with Kata We rely on kata-containers/kata-deploy to create the vaccel-kata-deploy daemon. Our fork repo can be found on cloudkernels/packaging . We are working on building a Kata Containers release with vAccel support. Label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Create service account and cluster role for the kata-deploy daemon $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-rbac/base/kata-rbac.yaml Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml # or for k3s $ k3s kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/k3s?ref=vaccel-dev The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m ... ... Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl -n kube-system logs kata-deploy-575tm ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before as it already contains jetson inference models. $ kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full?ref=vaccel-dev # or for k3s $ k3s kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full-k3s?ref=vaccel-dev Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml Deploy an image classification function as a Service The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl create -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/examples/web-classify.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m Run a curl command (from a cluster node) like the following to send your POST request to the Service web-classify-kata-fc (to access the Service from outside the cluster use nodePort or deploy an ingress route) $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST 10.43.214.52:80/classify --data-binary @- And see the result of the image classification! --2021-02-05 20:17:15-- https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg Resolving pbs.twimg.com (pbs.twimg.com)... 2606:2800:134:fa2:1627:1fe:edb:1665, 192.229.233.50 Connecting to pbs.twimg.com (pbs.twimg.com)|2606:2800:134:fa2:1627:1fe:edb:1665|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 12605 (12K) [image/jpeg] Saving to: 'STDOUT' - 100%[================================================>] 12.31K --.-KB/s in 0.04s 2021-02-05 20:17:15 (296 KB/s) - written to stdout [12605/12605] [\"web-classify-kata-fc-567bddccc4-s79b5\"]: \"29.761% wall clock\" Cleanup everything Delete the web-classify-fc deployment and service: $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/examples/web-classify.yaml Delete the daemon: (removes artifacts from host paths /opt/vaccel & /opt/kata and restores containerd configuration) $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml # or for k3s $ k3s kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/k3s?ref=vaccel-dev or in case you deployed the full vAccel overlay: $ kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full?ref=vaccel-dev # or for k3s $ k3s kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full-k3s?ref=vaccel-dev Reset the runtime and remove kata related labels from nodes: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-cleanup/base/kata-cleanup.yaml # or for k3s $ k3s kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-cleanup/overlays/k3s?ref=vaccel-dev Delete the kata-fc RuntimeClass and the rbac $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-rbac/base/kata-rbac.yaml Delete the cleanup daemon $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-cleanup/base/kata-cleanup.yaml # or for k3s $ k3s kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-cleanup/overlays/k3s?ref=vaccel-dev Remove vaccel=true from each node $ kubectl label nodes <your-node-name> vaccel=true-","title":"vAccel on k8s using Kata-containers & Firecracker"},{"location":"k8s/kata/#vaccel-on-k8s-using-kata-containers-firecracker","text":"","title":"vAccel on k8s using Kata-containers &amp; Firecracker"},{"location":"k8s/kata/#prerequisites","text":"In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter ( info ) nvidia GPU which supports CUDA ( for now ) ( info ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries) ( info )","title":"Prerequisites"},{"location":"k8s/kata/#quick-start","text":"","title":"Quick start"},{"location":"k8s/kata/#deploy-vaccel-with-kata","text":"We rely on kata-containers/kata-deploy to create the vaccel-kata-deploy daemon. Our fork repo can be found on cloudkernels/packaging . We are working on building a Kata Containers release with vAccel support. Label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Create service account and cluster role for the kata-deploy daemon $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-rbac/base/kata-rbac.yaml Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml # or for k3s $ k3s kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/k3s?ref=vaccel-dev The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m ... ... Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl -n kube-system logs kata-deploy-575tm ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before as it already contains jetson inference models. $ kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full?ref=vaccel-dev # or for k3s $ k3s kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full-k3s?ref=vaccel-dev Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml","title":"Deploy vAccel with Kata"},{"location":"k8s/kata/#deploy-an-image-classification-function-as-a-service","text":"The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl create -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/examples/web-classify.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m Run a curl command (from a cluster node) like the following to send your POST request to the Service web-classify-kata-fc (to access the Service from outside the cluster use nodePort or deploy an ingress route) $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST 10.43.214.52:80/classify --data-binary @- And see the result of the image classification! --2021-02-05 20:17:15-- https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg Resolving pbs.twimg.com (pbs.twimg.com)... 2606:2800:134:fa2:1627:1fe:edb:1665, 192.229.233.50 Connecting to pbs.twimg.com (pbs.twimg.com)|2606:2800:134:fa2:1627:1fe:edb:1665|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 12605 (12K) [image/jpeg] Saving to: 'STDOUT' - 100%[================================================>] 12.31K --.-KB/s in 0.04s 2021-02-05 20:17:15 (296 KB/s) - written to stdout [12605/12605] [\"web-classify-kata-fc-567bddccc4-s79b5\"]: \"29.761% wall clock\"","title":"Deploy an image classification function as a Service"},{"location":"k8s/kata/#cleanup-everything","text":"Delete the web-classify-fc deployment and service: $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/examples/web-classify.yaml Delete the daemon: (removes artifacts from host paths /opt/vaccel & /opt/kata and restores containerd configuration) $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml # or for k3s $ k3s kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/k3s?ref=vaccel-dev or in case you deployed the full vAccel overlay: $ kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full?ref=vaccel-dev # or for k3s $ k3s kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full-k3s?ref=vaccel-dev Reset the runtime and remove kata related labels from nodes: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-cleanup/base/kata-cleanup.yaml # or for k3s $ k3s kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-cleanup/overlays/k3s?ref=vaccel-dev Delete the kata-fc RuntimeClass and the rbac $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-rbac/base/kata-rbac.yaml Delete the cleanup daemon $ kubectl delete -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-cleanup/base/kata-cleanup.yaml # or for k3s $ k3s kubectl delete -k github.com/cloudkernels/packaging/kata-deploy/kata-cleanup/overlays/k3s?ref=vaccel-dev Remove vaccel=true from each node $ kubectl label nodes <your-node-name> vaccel=true-","title":"Cleanup everything"},{"location":"unikernels/unikraft/","text":"To run an image classification app in Unikraft using vAccel, we need: an NVIDIA GPU which supports CUDA the appropriate device drivers jetson-inference As long as we have the above depedencies, we can proceed setting up the vAccel environment for our example. A comprehensive walk through on installing the above dependencies is given in Jetson-inference . Additionally, we will need to set up 3 components: The vAccel Runtime system, vAccelrt on the bost A hypervisor with vAccel support, in our case QEMU and of course Unikraft To make things easier we have created some scripts and Docketfiles which produce a vAccel enabled QEMU and a Unikraft unikernel. You can get them from this repo. The first 2 components can be built using the build.sh script with the -u option, like: bash build.sh -u The last component can be created using the build_guest.sh script with the -u option, like: bash build_guest.sh -u After succefully building all components we can just fire up our unikernel using the command: bash run.sh This command will start Unikraft over QEMU classifying the image dog_0.jpg under guest/data/ directory of this repo. Let's take a closer look on what is happening inside the containers and how we can perform each step by ourselves. Setting up vAccert for the host vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case the backend plugin will be an NVIDIA GPU, and more specifically, jetson-inference, using TensorRT. As a result we need to build the Jetson plugin for vAccelrt. At first let's get the source code: git clone --recursive https://github.com/cloudkernels/vaccelrt.git cd vaccelrt As we mentioned before, we will also build the jetson plugin and we will specify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so Build QEMU with vAccel support For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. First lets get the source code: git clone --recursive https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side! Build the Unikraft application We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but our fork. In our fork we have added an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under the build directory. Fire it up! After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run our unikernel using the command below: LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"How to run a vAccel application using Unikraft"},{"location":"unikernels/unikraft/#setting-up-vaccert-for-the-host","text":"vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case the backend plugin will be an NVIDIA GPU, and more specifically, jetson-inference, using TensorRT. As a result we need to build the Jetson plugin for vAccelrt. At first let's get the source code: git clone --recursive https://github.com/cloudkernels/vaccelrt.git cd vaccelrt As we mentioned before, we will also build the jetson plugin and we will specify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so","title":"Setting up vAccert for the host"},{"location":"unikernels/unikraft/#build-qemu-with-vaccel-support","text":"For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. First lets get the source code: git clone --recursive https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side!","title":"Build QEMU with vAccel support"},{"location":"unikernels/unikraft/#build-the-unikraft-application","text":"We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but our fork. In our fork we have added an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under the build directory.","title":"Build the Unikraft application"},{"location":"unikernels/unikraft/#fire-it-up","text":"After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run our unikernel using the command below: LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"Fire it up!"}]}
{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to vAccel vAccel is a framework offering hardware acceleration primitive with focus on portability. It exposes to programmers \"accelerate-able\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. QEMU and AWS Firecracker VMMs are currently supported compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale. vAccel design vAccel runtime stack The core component of vAccel is the vaccel runtime (vAccelRT). vAccelRT is designed in a modular way. The core runtime exposes the vAccel API to user applications and dispatches requests to one of many backend plugins , which are the components that implement the vAccel API on a particular hardware accelerator. The user application links against the core runtime library and the plugin modules are loaded at runtime by the runtime. This workflow decouples the application from the hardware accelerator-specific parts of the stack, allowing for seamless migration of the same binary to different platforms with different accelerator capabilities, without the need to recomplile user code. Currently, we support acceleration plugins for: Jetson inference framework, for acceleration of ML operation on Nvidia GPUs. Google Coral TPU , for acceleration on Coral TPUs. Hardware acceleration in Virtual Machines Hardware acceleration for virtualized guests is a difficult problem to tackle. Typical solutions involve device pass-through or paravirtual drivers that expose hardware semantics inside the guest. vAccel differentiates itself from these approaches by exposing coarse grain \"acceleratable\" functions in the guest over a custom transport layer. The semantics of the transport layer are hidden from the programmer. A vAccel application that runs on baremetal with an Nvidia GPU can run as is inside a VM using our appropriate VirtIO backend plugin. We have implemented the necessary parts for our VirtIO driver in our forks of QEMU and Firecracker hypervisors. Performance vAccel performance overhead of VM execution compared to bare-metal The above figure depicts a performance comparison of the image classification operation of a vAccel application running inside a Firecracker VM using the Jetson inference plugin comparing to an execution of the same operation using the Jetson inference framework natively without any vAccel code, running directly on the host. The performance overhead of our stack is less that 5% of the native execution time across a range of image sizes. We will be updating these performance results regularly with more operations and plugins.","title":"Home"},{"location":"#welcome-to-vaccel","text":"vAccel is a framework offering hardware acceleration primitive with focus on portability. It exposes to programmers \"accelerate-able\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. QEMU and AWS Firecracker VMMs are currently supported compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale.","title":"Welcome to vAccel"},{"location":"#vaccel-design","text":"vAccel runtime stack The core component of vAccel is the vaccel runtime (vAccelRT). vAccelRT is designed in a modular way. The core runtime exposes the vAccel API to user applications and dispatches requests to one of many backend plugins , which are the components that implement the vAccel API on a particular hardware accelerator. The user application links against the core runtime library and the plugin modules are loaded at runtime by the runtime. This workflow decouples the application from the hardware accelerator-specific parts of the stack, allowing for seamless migration of the same binary to different platforms with different accelerator capabilities, without the need to recomplile user code. Currently, we support acceleration plugins for: Jetson inference framework, for acceleration of ML operation on Nvidia GPUs. Google Coral TPU , for acceleration on Coral TPUs.","title":"vAccel design"},{"location":"#hardware-acceleration-in-virtual-machines","text":"Hardware acceleration for virtualized guests is a difficult problem to tackle. Typical solutions involve device pass-through or paravirtual drivers that expose hardware semantics inside the guest. vAccel differentiates itself from these approaches by exposing coarse grain \"acceleratable\" functions in the guest over a custom transport layer. The semantics of the transport layer are hidden from the programmer. A vAccel application that runs on baremetal with an Nvidia GPU can run as is inside a VM using our appropriate VirtIO backend plugin. We have implemented the necessary parts for our VirtIO driver in our forks of QEMU and Firecracker hypervisors.","title":"Hardware acceleration in Virtual Machines"},{"location":"#performance","text":"vAccel performance overhead of VM execution compared to bare-metal The above figure depicts a performance comparison of the image classification operation of a vAccel application running inside a Firecracker VM using the Jetson inference plugin comparing to an execution of the same operation using the Jetson inference framework natively without any vAccel code, running directly on the host. The performance overhead of our stack is less that 5% of the native execution time across a range of image sizes. We will be updating these performance results regularly with more operations and plugins.","title":"Performance"},{"location":"coral/","text":"Google Coral TPU To build the vAccel Google Coral TPU plugin, we will use a simple image classification example based on tflite To execute an image classification operation on the Google Coral TPU, we first need to implement the operation using one of the available acceleration frameworks and then register this function to the vAccel plugin we are building. To achieve this, we patch a tflite example for Google coral, and build the available image classification function as a shared library, exposing the following API to the relevant plugin: int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) Patch the code The first step in order to build the library is to get the example code from the Google Coral github repo: git clone https://github.com/google-coral/tflite Then we need to apply the following patch ( libify_classify.patch ) to the relevant example: diff --git a/cpp/examples/classification/classify.cc b/cpp/examples/classification/classify.cc index 69dfe04..1129976 100644 --- a/cpp/examples/classification/classify.cc +++ b/cpp/examples/classification/classify.cc @@ -22,6 +22,72 @@ int32_t ToInt32(const char p[4]) { return (p[3] << 24) | (p[2] << 16) | (p[1] << 8) | p[0]; } +std::vector<uint8_t> ReadBmpImageFromBuf(char* buf, + int* out_width = nullptr, + int* out_height = nullptr, + int* out_channels = nullptr) { + char header[kBmpHeaderSize]; + memcpy(header, buf, sizeof(header)); + + const char* file_header = header; + const char* info_header = header + kBmpFileHeaderSize; + char *ptr = buf + sizeof(header); + + if (file_header[0] != 'B' || file_header[1] != 'M') + return {}; // Invalid file type. + + const int channels = info_header[14] / 8; + if (channels != 1 && channels != 3) return {}; // Unsupported bits per pixel. + + if (ToInt32(&info_header[16]) != 0) return {}; // Unsupported compression. + + uint32_t offset = ToInt32(&file_header[10]); +#if 0 + if (offset > kBmpHeaderSize)// && + //!file.seekg(offset - kBmpHeaderSize, std::ios::cur)) + return {}; // Seek failed. +#endif + + offset -= kBmpHeaderSize; + //printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + //printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + //printf(\"offset:%d\\n\", offset); + ptr = ptr+offset; + int width = ToInt32(&info_header[4]); + if (width < 0) return {}; // Invalid width. + + int height = ToInt32(&info_header[8]); + const bool top_down = height < 0; + if (top_down) height = -height; + + const int line_bytes = width * channels; + const int line_padding_bytes = + 4 * ((8 * channels * width + 31) / 32) - line_bytes; + std::vector<uint8_t> image(line_bytes * height); + for (int i = 0; i < height; ++i) { + uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + memcpy(line, ptr, line_bytes); +#if 0 + if (!file.read(reinterpret_cast<char*>(line), line_bytes)) + return {}; // Read failed. + if (!file.seekg(line_padding_bytes, std::ios::cur)) + return {}; // Seek failed. +#endif + //printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); + ptr = ptr + line_padding_bytes; + ptr = ptr + line_bytes; + if (channels == 3) { + for (int j = 0; j < width; ++j) std::swap(line[3 * j], line[3 * j + 2]); + } + } + + if (out_width) *out_width = width; + if (out_height) *out_height = height; + if (out_channels) *out_channels = channels; + return image; +} + + std::vector<uint8_t> ReadBmpImage(const char* filename, int* out_width = nullptr, int* out_height = nullptr, @@ -50,6 +116,9 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, !file.seekg(offset - kBmpHeaderSize, std::ios::cur)) return {}; // Seek failed. + printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + printf(\"offset:%d\\n\", offset); int width = ToInt32(&info_header[4]); if (width < 0) return {}; // Invalid width. @@ -63,6 +132,7 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, std::vector<uint8_t> image(line_bytes * height); for (int i = 0; i < height; ++i) { uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); if (!file.read(reinterpret_cast<char*>(line), line_bytes)) return {}; // Read failed. if (!file.seekg(line_padding_bytes, std::ios::cur)) @@ -117,18 +187,12 @@ std::vector<std::pair<int, float>> Sort(const std::vector<float>& scores, } } // namespace -int main(int argc, char* argv[]) { - if (argc != 5) { - std::cerr << argv[0] - << \" <model_file> <label_file> <image_file> <threshold>\" - << std::endl; - return 1; - } - - const std::string model_file = argv[1]; - const std::string label_file = argv[2]; - const std::string image_file = argv[3]; - const float threshold = std::stof(argv[4]); +extern \"C\" int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) +{ + const std::string model_file (model_file_ptr); + const std::string label_file (label_file_ptr); + const std::string image_file (image_file_ptr); + const float threshold = thres; // Find TPU device. size_t num_devices; @@ -151,7 +215,8 @@ int main(int argc, char* argv[]) { // Load image. int image_bpp, image_width, image_height; auto image = - ReadBmpImage(image_file.c_str(), &image_width, &image_height, &image_bpp); + ReadBmpImageFromBuf(image_file_ptr, &image_width, &image_height, &image_bpp); + //ReadBmpImage(image_file_ptr, &image_width, &image_height, &image_bpp); if (image.empty()) { std::cerr << \"Cannot read image from \" << image_file << std::endl; return 1; @@ -174,7 +239,7 @@ int main(int argc, char* argv[]) { auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0); - interpreter->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate}); + interpreter->ModifyGraphWithDelegate(delegate); // Allocate tensors. if (interpreter->AllocateTensors() != kTfLiteOk) { @@ -204,9 +269,13 @@ int main(int argc, char* argv[]) { // Get interpreter output. auto results = Sort(Dequantize(*interpreter->output_tensor(0)), threshold); + *tags = strdup(GetLabel(labels, results[0].first).c_str()); + *out_len = strlen(*tags); +#if 0 for (auto& result : results) std::cout << std::setw(7) << std::fixed << std::setprecision(5) << result.second << GetLabel(labels, result.first) << std::endl; +#endif return 0; } cd tflite patch -p1 < ../libify_classify.path Alternatively you could just clone from our forked repo: git clone https://github.com/nubificus/tflite -b vaccel Then, we need to install the requirements & build tflite for google coral (could take some time, so please be patient ;-)). Download the latest Edge TPU runtime You can grab the latest Edge TPU runtime archive from https://coral.ai/software/ . Then extract it next to the Makefile: cd tflite/cpp/examples/classification/ wget https://dl.google.com/coral/edgetpu_api/edgetpu_runtime_20200710.zip Download and build Tensorflow git clone https://github.com/tensorflow/tensorflow.git tensorflow/tensorflow/lite/tools/make/download_dependencies.sh tensorflow/tensorflow/lite/tools/make/build_lib.sh Build the shared library Finally, build the classify shared object that we will then link to the vAccel Google Coral TPU plugin. g++ -shared -fPIC -std=c++11 classify.cc -o /usr/local/lib/libclassify.so -I/data/tflite/cpp/examples/classification/edgetpu_runtime/libedgetpu/ -Itensorflow/ -Itensorflow//tensorflow/lite/tools/make/downloads/flatbuffers/include -Ltensorflow//tensorflow/lite/tools/make/gen/generic-aarch64_armv8-a/lib -Ledgetpu_runtime/libedgetpu/direct/aarch64/ -l:libedgetpu.so.1.0 -lpthread -lm -ldl tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a Build vAccelRT plugin Following the generic process for vAccelRT building (from Building vAccelRT ) we now have to enable the vAccel Google Coral Edge TPU plugin. In short, the previous steps for vAccelRT were the following 1 : git clone --recursive https://github.com/cloudkernels/vaccelrt -b feat_gcoral_plugin cd vaccelrt mkdir build cd build cmake ../ The additional step now is to enable the plugin via cmake: cd vaccelrt mkdir build_coral && cd build_coral cmake ../ -DBUILD_PLUGIN_CORAL=ON The output of the above command should be something like the following: -- The C compiler identification is GNU 8.3.0 -- The CXX compiler identification is GNU 8.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral/googletest-download Scanning dependencies of target googletest [ 11%] Creating directories for 'googletest' [ 22%] Performing download step (git clone) for 'googletest' Cloning into 'googletest-src'... Note: checking out 'release-1.8.1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at 2fe3bd99 Merge pull request #1433 from dsacre/fix-clang-warnings [ 33%] No patch step for 'googletest' [ 44%] Performing update step for 'googletest' [ 55%] No configure step for 'googletest' [ 66%] No build step for 'googletest' [ 77%] No install step for 'googletest' [ 88%] No test step for 'googletest' [100%] Completed 'googletest' [100%] Built target googletest -- Found PythonInterp: /usr/bin/python (found version \"2.7.16\") -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Found rt: /usr/lib/aarch64-linux-gnu/librt.so Include directories: /data/vaccelrt/src -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral Finally, building the plugin is as simple as issueing a make command: make The plugin is available at: vaccelrt/build_coral/plugins/coral/libvaccel-coral.so Extra Files To run an example on the Google Coral Edge TPU, follow the instructions available at Running a simple example . The files needed for classification are: mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt available for download from: https://github.com/google-coral/test_data . Support for Google Coral is WiP , and will be available shortly in the main branch. \u21a9","title":"Google Coral TPU"},{"location":"coral/#google-coral-tpu","text":"To build the vAccel Google Coral TPU plugin, we will use a simple image classification example based on tflite To execute an image classification operation on the Google Coral TPU, we first need to implement the operation using one of the available acceleration frameworks and then register this function to the vAccel plugin we are building. To achieve this, we patch a tflite example for Google coral, and build the available image classification function as a shared library, exposing the following API to the relevant plugin: int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len)","title":"Google Coral TPU"},{"location":"coral/#patch-the-code","text":"The first step in order to build the library is to get the example code from the Google Coral github repo: git clone https://github.com/google-coral/tflite Then we need to apply the following patch ( libify_classify.patch ) to the relevant example: diff --git a/cpp/examples/classification/classify.cc b/cpp/examples/classification/classify.cc index 69dfe04..1129976 100644 --- a/cpp/examples/classification/classify.cc +++ b/cpp/examples/classification/classify.cc @@ -22,6 +22,72 @@ int32_t ToInt32(const char p[4]) { return (p[3] << 24) | (p[2] << 16) | (p[1] << 8) | p[0]; } +std::vector<uint8_t> ReadBmpImageFromBuf(char* buf, + int* out_width = nullptr, + int* out_height = nullptr, + int* out_channels = nullptr) { + char header[kBmpHeaderSize]; + memcpy(header, buf, sizeof(header)); + + const char* file_header = header; + const char* info_header = header + kBmpFileHeaderSize; + char *ptr = buf + sizeof(header); + + if (file_header[0] != 'B' || file_header[1] != 'M') + return {}; // Invalid file type. + + const int channels = info_header[14] / 8; + if (channels != 1 && channels != 3) return {}; // Unsupported bits per pixel. + + if (ToInt32(&info_header[16]) != 0) return {}; // Unsupported compression. + + uint32_t offset = ToInt32(&file_header[10]); +#if 0 + if (offset > kBmpHeaderSize)// && + //!file.seekg(offset - kBmpHeaderSize, std::ios::cur)) + return {}; // Seek failed. +#endif + + offset -= kBmpHeaderSize; + //printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + //printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + //printf(\"offset:%d\\n\", offset); + ptr = ptr+offset; + int width = ToInt32(&info_header[4]); + if (width < 0) return {}; // Invalid width. + + int height = ToInt32(&info_header[8]); + const bool top_down = height < 0; + if (top_down) height = -height; + + const int line_bytes = width * channels; + const int line_padding_bytes = + 4 * ((8 * channels * width + 31) / 32) - line_bytes; + std::vector<uint8_t> image(line_bytes * height); + for (int i = 0; i < height; ++i) { + uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + memcpy(line, ptr, line_bytes); +#if 0 + if (!file.read(reinterpret_cast<char*>(line), line_bytes)) + return {}; // Read failed. + if (!file.seekg(line_padding_bytes, std::ios::cur)) + return {}; // Seek failed. +#endif + //printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); + ptr = ptr + line_padding_bytes; + ptr = ptr + line_bytes; + if (channels == 3) { + for (int j = 0; j < width; ++j) std::swap(line[3 * j], line[3 * j + 2]); + } + } + + if (out_width) *out_width = width; + if (out_height) *out_height = height; + if (out_channels) *out_channels = channels; + return image; +} + + std::vector<uint8_t> ReadBmpImage(const char* filename, int* out_width = nullptr, int* out_height = nullptr, @@ -50,6 +116,9 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, !file.seekg(offset - kBmpHeaderSize, std::ios::cur)) return {}; // Seek failed. + printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + printf(\"offset:%d\\n\", offset); int width = ToInt32(&info_header[4]); if (width < 0) return {}; // Invalid width. @@ -63,6 +132,7 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, std::vector<uint8_t> image(line_bytes * height); for (int i = 0; i < height; ++i) { uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); if (!file.read(reinterpret_cast<char*>(line), line_bytes)) return {}; // Read failed. if (!file.seekg(line_padding_bytes, std::ios::cur)) @@ -117,18 +187,12 @@ std::vector<std::pair<int, float>> Sort(const std::vector<float>& scores, } } // namespace -int main(int argc, char* argv[]) { - if (argc != 5) { - std::cerr << argv[0] - << \" <model_file> <label_file> <image_file> <threshold>\" - << std::endl; - return 1; - } - - const std::string model_file = argv[1]; - const std::string label_file = argv[2]; - const std::string image_file = argv[3]; - const float threshold = std::stof(argv[4]); +extern \"C\" int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) +{ + const std::string model_file (model_file_ptr); + const std::string label_file (label_file_ptr); + const std::string image_file (image_file_ptr); + const float threshold = thres; // Find TPU device. size_t num_devices; @@ -151,7 +215,8 @@ int main(int argc, char* argv[]) { // Load image. int image_bpp, image_width, image_height; auto image = - ReadBmpImage(image_file.c_str(), &image_width, &image_height, &image_bpp); + ReadBmpImageFromBuf(image_file_ptr, &image_width, &image_height, &image_bpp); + //ReadBmpImage(image_file_ptr, &image_width, &image_height, &image_bpp); if (image.empty()) { std::cerr << \"Cannot read image from \" << image_file << std::endl; return 1; @@ -174,7 +239,7 @@ int main(int argc, char* argv[]) { auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0); - interpreter->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate}); + interpreter->ModifyGraphWithDelegate(delegate); // Allocate tensors. if (interpreter->AllocateTensors() != kTfLiteOk) { @@ -204,9 +269,13 @@ int main(int argc, char* argv[]) { // Get interpreter output. auto results = Sort(Dequantize(*interpreter->output_tensor(0)), threshold); + *tags = strdup(GetLabel(labels, results[0].first).c_str()); + *out_len = strlen(*tags); +#if 0 for (auto& result : results) std::cout << std::setw(7) << std::fixed << std::setprecision(5) << result.second << GetLabel(labels, result.first) << std::endl; +#endif return 0; } cd tflite patch -p1 < ../libify_classify.path Alternatively you could just clone from our forked repo: git clone https://github.com/nubificus/tflite -b vaccel Then, we need to install the requirements & build tflite for google coral (could take some time, so please be patient ;-)).","title":"Patch the code"},{"location":"coral/#download-the-latest-edge-tpu-runtime","text":"You can grab the latest Edge TPU runtime archive from https://coral.ai/software/ . Then extract it next to the Makefile: cd tflite/cpp/examples/classification/ wget https://dl.google.com/coral/edgetpu_api/edgetpu_runtime_20200710.zip","title":"Download the latest Edge TPU runtime"},{"location":"coral/#download-and-build-tensorflow","text":"git clone https://github.com/tensorflow/tensorflow.git tensorflow/tensorflow/lite/tools/make/download_dependencies.sh tensorflow/tensorflow/lite/tools/make/build_lib.sh","title":"Download and build Tensorflow"},{"location":"coral/#build-the-shared-library","text":"Finally, build the classify shared object that we will then link to the vAccel Google Coral TPU plugin. g++ -shared -fPIC -std=c++11 classify.cc -o /usr/local/lib/libclassify.so -I/data/tflite/cpp/examples/classification/edgetpu_runtime/libedgetpu/ -Itensorflow/ -Itensorflow//tensorflow/lite/tools/make/downloads/flatbuffers/include -Ltensorflow//tensorflow/lite/tools/make/gen/generic-aarch64_armv8-a/lib -Ledgetpu_runtime/libedgetpu/direct/aarch64/ -l:libedgetpu.so.1.0 -lpthread -lm -ldl tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a","title":"Build the shared library"},{"location":"coral/#build-vaccelrt-plugin","text":"Following the generic process for vAccelRT building (from Building vAccelRT ) we now have to enable the vAccel Google Coral Edge TPU plugin. In short, the previous steps for vAccelRT were the following 1 : git clone --recursive https://github.com/cloudkernels/vaccelrt -b feat_gcoral_plugin cd vaccelrt mkdir build cd build cmake ../ The additional step now is to enable the plugin via cmake: cd vaccelrt mkdir build_coral && cd build_coral cmake ../ -DBUILD_PLUGIN_CORAL=ON The output of the above command should be something like the following: -- The C compiler identification is GNU 8.3.0 -- The CXX compiler identification is GNU 8.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral/googletest-download Scanning dependencies of target googletest [ 11%] Creating directories for 'googletest' [ 22%] Performing download step (git clone) for 'googletest' Cloning into 'googletest-src'... Note: checking out 'release-1.8.1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at 2fe3bd99 Merge pull request #1433 from dsacre/fix-clang-warnings [ 33%] No patch step for 'googletest' [ 44%] Performing update step for 'googletest' [ 55%] No configure step for 'googletest' [ 66%] No build step for 'googletest' [ 77%] No install step for 'googletest' [ 88%] No test step for 'googletest' [100%] Completed 'googletest' [100%] Built target googletest -- Found PythonInterp: /usr/bin/python (found version \"2.7.16\") -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Found rt: /usr/lib/aarch64-linux-gnu/librt.so Include directories: /data/vaccelrt/src -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral Finally, building the plugin is as simple as issueing a make command: make The plugin is available at: vaccelrt/build_coral/plugins/coral/libvaccel-coral.so","title":"Build vAccelRT plugin"},{"location":"coral/#extra-files","text":"To run an example on the Google Coral Edge TPU, follow the instructions available at Running a simple example . The files needed for classification are: mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt available for download from: https://github.com/google-coral/test_data . Support for Google Coral is WiP , and will be available shortly in the main branch. \u21a9","title":"Extra Files"},{"location":"examples/","text":"vAccel on Google Coral Edge TPU Running compute-intensive workloads on low-power/Edge devices is a challenging task, especially when multiple factors are being considered, such as security, isolation, performance etc. To this end, one of our initial goals in developing the vAccel framework is to facilitate the acceleration of ML inference operations when executing on low-power devices such as an NVIDIA Jetson Nano. Below we walk through the process of running a simple image classification operation from an Amazon Firecracker VM, using the Maxwell GPU present in the Jetson compute module. Building a vaccel application We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images Running the example Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the Google Coral plugin which implements the image classification operation using libedgetpu and tflite. You can get more info on building this plugin in the relevant section of the currect documentation. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so Finally, the classification application needs the mobilenet model file & labels in the current working path. wget https://github.com/google-coral/test_data/raw/c21de4450f88a20ac5968628d375787745932a5a/mobilenet_v1_1.0_224_quant_edgetpu.tflite wget https://raw.githubusercontent.com/google-coral/test_data/c21de4450f88a20ac5968628d375787745932a5a/imagenet_labels.txt $ ls classify resized_cat.bmp mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt $ ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat Running the same example on an AWS Firecracker VM As mentioned, with vAccel we are able to use the virtio-accel backend while in a VM, to forward specific acceleration operations to the Host OS which, in turn, will execute them on real hardware. Below, we summarize the process to use the Google Coral Edge TPU image classification hardware acceleration functionality from a Firecracker VM. First, we need to have a basic set of binaries to boot a VM and use the virtio-accel backend of vAccelRT. In short, the process to follow is given below: Prepare the system to boot a Firecracker VM Build Firecracker To build the vAccel Firecracker port, we do the following on our aarch64 host: git clone https://github.com/cloudkernels/firecracker -b vaccel-v0.23 cd firecracker/tools/devctr docker build -t nubificus/fcuvm:v0.23 -f Dockerfile.aarch64 cd ../../ tools/devtool build --release -l gnu That should output a binary at build/cargo_target/aarch64-unknown-linux-gnu/release/firecracker relative to the firecracker source tree. Building the kernel & rootfs To facilitate the process of building a kernel image and rootfs for vAccel, you can follow the instructions on the aarch64 tools repo : git clone https://github.com/nubificus/fc-aarch64-guest-build cd fc-aarch64-guest-build bash build.sh If all went well, there should be an output folder with the following files: # ls output Image rootfs.img config_vaccel.json Boot the Firecracker VM Then, using the binaries from the previous steps we spawn a Firecracker VM: export LD_LIBRARY_PATH=${HOME}/.local/lib export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 The output of the above command should be something like the following: ./firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 [ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x410fd034] [ 0.000000] Linux version 4.20.0 (root@buildkitsandbox) (gcc version 8.3.0 (Debian 8.3.0-6)) #1 SMP Thu Dec 3 23:58:32 UTC 2020 [ 0.000000] Machine model: linux,dummy-virt [ 0.000000] earlycon: uart0 at MMIO 0x0000000040003000 (options '') [ 0.000000] printk: bootconsole [uart0] enabled [ 0.000000] efi: Getting EFI parameters from FDT: [ 0.000000] efi: UEFI not found. [ 0.000000] NUMA: No NUMA configuration found [ 0.000000] NUMA: Faking a node at [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] NUMA: NODE_DATA [mem 0x8fedbf00-0x8fef4fff] [ 0.000000] Zone ranges: [ 0.000000] DMA32 [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Normal empty [ 0.000000] Movable zone start for each node [ 0.000000] Early memory node ranges [ 0.000000] node 0: [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Initmem setup node 0 [mem 0x0000000080000000-0x000000008fffffff] [snipped] [ OK ] Reached target Multi-User System. [ OK ] Reached target Graphical Interface. Starting Update UTMP about System Runlevel Changes... [ OK ] Finished Update UTMP about System Runlevel Changes. Ubuntu 20.04.1 LTS localhost.localdomain ttyS0 localhost login: Perform the operation Go ahead and login as root and execute the classification command: localhost login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Sat Feb 6 10:00:59 UTC 2021 on ttyS0 root@localhost:~# ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat root@localhost:~# There you go! Using the Google Coral Edge TPU hardware from a Firecracker VM we were able to perform ML inference!","title":"Examples"},{"location":"examples/#vaccel-on-google-coral-edge-tpu","text":"Running compute-intensive workloads on low-power/Edge devices is a challenging task, especially when multiple factors are being considered, such as security, isolation, performance etc. To this end, one of our initial goals in developing the vAccel framework is to facilitate the acceleration of ML inference operations when executing on low-power devices such as an NVIDIA Jetson Nano. Below we walk through the process of running a simple image classification operation from an Amazon Firecracker VM, using the Maxwell GPU present in the Jetson compute module.","title":"vAccel on Google Coral Edge TPU"},{"location":"examples/#building-a-vaccel-application","text":"We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images","title":"Building a vaccel application"},{"location":"examples/#running-the-example","text":"Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the Google Coral plugin which implements the image classification operation using libedgetpu and tflite. You can get more info on building this plugin in the relevant section of the currect documentation. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so Finally, the classification application needs the mobilenet model file & labels in the current working path. wget https://github.com/google-coral/test_data/raw/c21de4450f88a20ac5968628d375787745932a5a/mobilenet_v1_1.0_224_quant_edgetpu.tflite wget https://raw.githubusercontent.com/google-coral/test_data/c21de4450f88a20ac5968628d375787745932a5a/imagenet_labels.txt $ ls classify resized_cat.bmp mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt $ ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat","title":"Running the example"},{"location":"examples/#running-the-same-example-on-an-aws-firecracker-vm","text":"As mentioned, with vAccel we are able to use the virtio-accel backend while in a VM, to forward specific acceleration operations to the Host OS which, in turn, will execute them on real hardware. Below, we summarize the process to use the Google Coral Edge TPU image classification hardware acceleration functionality from a Firecracker VM. First, we need to have a basic set of binaries to boot a VM and use the virtio-accel backend of vAccelRT. In short, the process to follow is given below:","title":"Running the same example on an AWS Firecracker VM"},{"location":"examples/#prepare-the-system-to-boot-a-firecracker-vm","text":"","title":"Prepare the system to boot a Firecracker VM"},{"location":"examples/#build-firecracker","text":"To build the vAccel Firecracker port, we do the following on our aarch64 host: git clone https://github.com/cloudkernels/firecracker -b vaccel-v0.23 cd firecracker/tools/devctr docker build -t nubificus/fcuvm:v0.23 -f Dockerfile.aarch64 cd ../../ tools/devtool build --release -l gnu That should output a binary at build/cargo_target/aarch64-unknown-linux-gnu/release/firecracker relative to the firecracker source tree.","title":"Build Firecracker"},{"location":"examples/#building-the-kernel-rootfs","text":"To facilitate the process of building a kernel image and rootfs for vAccel, you can follow the instructions on the aarch64 tools repo : git clone https://github.com/nubificus/fc-aarch64-guest-build cd fc-aarch64-guest-build bash build.sh If all went well, there should be an output folder with the following files: # ls output Image rootfs.img config_vaccel.json","title":"Building the kernel &amp; rootfs"},{"location":"examples/#boot-the-firecracker-vm","text":"Then, using the binaries from the previous steps we spawn a Firecracker VM: export LD_LIBRARY_PATH=${HOME}/.local/lib export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-coral.so firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 The output of the above command should be something like the following: ./firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0 [ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x410fd034] [ 0.000000] Linux version 4.20.0 (root@buildkitsandbox) (gcc version 8.3.0 (Debian 8.3.0-6)) #1 SMP Thu Dec 3 23:58:32 UTC 2020 [ 0.000000] Machine model: linux,dummy-virt [ 0.000000] earlycon: uart0 at MMIO 0x0000000040003000 (options '') [ 0.000000] printk: bootconsole [uart0] enabled [ 0.000000] efi: Getting EFI parameters from FDT: [ 0.000000] efi: UEFI not found. [ 0.000000] NUMA: No NUMA configuration found [ 0.000000] NUMA: Faking a node at [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] NUMA: NODE_DATA [mem 0x8fedbf00-0x8fef4fff] [ 0.000000] Zone ranges: [ 0.000000] DMA32 [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Normal empty [ 0.000000] Movable zone start for each node [ 0.000000] Early memory node ranges [ 0.000000] node 0: [mem 0x0000000080000000-0x000000008fffffff] [ 0.000000] Initmem setup node 0 [mem 0x0000000080000000-0x000000008fffffff] [snipped] [ OK ] Reached target Multi-User System. [ OK ] Reached target Graphical Interface. Starting Update UTMP about System Runlevel Changes... [ OK ] Finished Update UTMP about System Runlevel Changes. Ubuntu 20.04.1 LTS localhost.localdomain ttyS0 localhost login:","title":"Boot the Firecracker VM"},{"location":"examples/#perform-the-operation","text":"Go ahead and login as root and execute the classification command: localhost login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Sat Feb 6 10:00:59 UTC 2021 on ttyS0 root@localhost:~# ./classify resized_cat.bmp 1 Initialized session with id: 1 Image size: 150666B classification tags: 286 Egyptian cat root@localhost:~# There you go! Using the Google Coral Edge TPU hardware from a Firecracker VM we were able to perform ML inference!","title":"Perform the operation"},{"location":"jetson/","text":"Jetson-inference vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload with the jetson-inference plugin, we will use a set of NVIDIA GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. The Jetson-inference vAccel plugin is based on jetson-inference , a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. Setup NVIDIA custom repos The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Install CUDA and TensorRT Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Prepare for building jetson-inference Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference Patch to support GTX 1030 & RTX 2060 To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda Building jetson-inference We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install","title":"Jetson-inference"},{"location":"jetson/#jetson-inference","text":"vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload with the jetson-inference plugin, we will use a set of NVIDIA GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. The Jetson-inference vAccel plugin is based on jetson-inference , a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system.","title":"Jetson-inference"},{"location":"jetson/#setup-nvidia-custom-repos","text":"The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update","title":"Setup NVIDIA custom repos"},{"location":"jetson/#install-cuda-and-tensorrt","text":"Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7","title":"Install CUDA and TensorRT"},{"location":"jetson/#prepare-for-building-jetson-inference","text":"Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference","title":"Prepare for building jetson-inference"},{"location":"jetson/#patch-to-support-gtx-1030-rtx-2060","text":"To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda","title":"Patch to support GTX 1030 &amp; RTX 2060"},{"location":"jetson/#building-jetson-inference","text":"We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install","title":"Building jetson-inference"},{"location":"kata-vaccel/","text":"vAccel on kata containers We have ported vAccel to the Kata Containers Runtime for Firecracker with containerd Shim v2. You can try it directly on Kubernetes following the docs ! Visit our fork repos to have a look on our WiP to fully integrate vAccel with Kata Containers on QEMU / Firecracker. vaccel-kata-runtime vaccel-kata-agent","title":"vAccel on kata containers"},{"location":"kata-vaccel/#vaccel-on-kata-containers","text":"We have ported vAccel to the Kata Containers Runtime for Firecracker with containerd Shim v2. You can try it directly on Kubernetes following the docs ! Visit our fork repos to have a look on our WiP to fully integrate vAccel with Kata Containers on QEMU / Firecracker. vaccel-kata-runtime vaccel-kata-agent","title":"vAccel on kata containers"},{"location":"quickstart/","text":"Quickstart This is a quick start guide for running a simple vAccel application on a x86 machine with an Nvidia GPU. We will build the vAccel stack and run a simple application both directly on a Linux host and inside a Firecracker VM. Building the vAccel stack The vAccel repo serves as a means to keep track of all the individual components needed to run a vAccel application. git clone https://github.com/cloudkernels/vaccel cd vaccel The repo includes a build script which will build all the components: The vAccelRT with the currently supported plugins . The virtio-accel kernel module which acts as the transport layer when running inside the Firecracker VM The forked Firecracker VMM which is patched to handle virtio-accel requests A rootfs and suitable vmlinux kernel image for booting the Firecracker VM An example application for testing our setup Building the Jetson inference plugin requires a working jetson-inference installation along with the corresponding CUDA environment on the machine. If you have it you can just run: ./build.sh all Note: while building the rootfs, this script will require your sudo password. If a jetson-inference setup is not available you can either follow this guide to build the vAccel jetson plugin and install the prerequisites on your host machine, or use our nubificus/vaccel-deps container image to build the necessary components, by running: ./build.sh -c all Running the application The above build process create two directories: build includes intermediate files of the building process and output includes the build artifacts we will need to run our example: cd output/debug ls bin include lib share Native execution Running on the host In case you have the jetson-inference framework installed on your host, we simply can execute: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Run the image classification example ./bin/classify ./share/images/dog_0.jpg 1 You should some logging from the jetson-inference framework and in the end of it you should get the classification tags of the image: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Note : The first time your run a classification with a model jetson-inference is performing some JIT steps to optimize the classification result, so you can expect increased execution time. The output of this operation is cached for subsequent executions. Running in a Firecracker VM build.sh created for us a rootfs image and a vmlinux kernel for booting a Firecracker VM. The rootfs has pre-installed the vAccel libraries, the example application the images and models. Additionally, it installed for us the following Firecracker configuration for booting the VM. In similar fashion as before, we launch the VM: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Launch the Firecracker VM ./bin/firecracker --api-sock fc.sock --config-file share/config_virtio_accel.json --seccomp-level 0 This will launch the Firecracker VM and it will result in a login prompt in the VM, to which we can login using the root user without a password. Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0 vaccel-guest login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Fri Feb 5 17:32:12 UTC 2021 on ttyS0 root@vaccel-guest:~# Once, in the prompt of the guest we setup the environment and run the example: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1 Note : In this case, we use the VirtIO plugin since we are inside the VM and we do not need to define the path to the pre-trained model. The latter is necessary only on the host where we use the jetson plugin. The result of the classification should be the same as before: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Docker execution If you don't have a Jetson-inference installation available on your machine, you can use our nubificus/vaccel-deps Docker image to run a vAccel application. You need to install the Nvidia container runtime following the instructions here , which allows you to use NVIDIA GPU devices inside Docker containers. Once, you're all setup with the nvidia runtime you can run the application: docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/classify share/images/dog_0.jpg 1 Similarly, we can launch a Firecracker VM inside the container. In this case, we need to add the --privileged flag to allow launching VMs inside the container and -it to be able to use the VM's console. docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ --privileged \\ -it \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/firecracker --api-sock fc.sock --config-file ./share/config_virtio_accel.json --seccomp-level 0 which we'll give us a console inside the VM, from which we can run our application the same way as we did before: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This is a quick start guide for running a simple vAccel application on a x86 machine with an Nvidia GPU. We will build the vAccel stack and run a simple application both directly on a Linux host and inside a Firecracker VM.","title":"Quickstart"},{"location":"quickstart/#building-the-vaccel-stack","text":"The vAccel repo serves as a means to keep track of all the individual components needed to run a vAccel application. git clone https://github.com/cloudkernels/vaccel cd vaccel The repo includes a build script which will build all the components: The vAccelRT with the currently supported plugins . The virtio-accel kernel module which acts as the transport layer when running inside the Firecracker VM The forked Firecracker VMM which is patched to handle virtio-accel requests A rootfs and suitable vmlinux kernel image for booting the Firecracker VM An example application for testing our setup Building the Jetson inference plugin requires a working jetson-inference installation along with the corresponding CUDA environment on the machine. If you have it you can just run: ./build.sh all Note: while building the rootfs, this script will require your sudo password. If a jetson-inference setup is not available you can either follow this guide to build the vAccel jetson plugin and install the prerequisites on your host machine, or use our nubificus/vaccel-deps container image to build the necessary components, by running: ./build.sh -c all","title":"Building the vAccel stack"},{"location":"quickstart/#running-the-application","text":"The above build process create two directories: build includes intermediate files of the building process and output includes the build artifacts we will need to run our example: cd output/debug ls bin include lib share","title":"Running the application"},{"location":"quickstart/#native-execution","text":"","title":"Native execution"},{"location":"quickstart/#running-on-the-host","text":"In case you have the jetson-inference framework installed on your host, we simply can execute: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Run the image classification example ./bin/classify ./share/images/dog_0.jpg 1 You should some logging from the jetson-inference framework and in the end of it you should get the classification tags of the image: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Note : The first time your run a classification with a model jetson-inference is performing some JIT steps to optimize the classification result, so you can expect increased execution time. The output of this operation is cached for subsequent executions.","title":"Running on the host"},{"location":"quickstart/#running-in-a-firecracker-vm","text":"build.sh created for us a rootfs image and a vmlinux kernel for booting a Firecracker VM. The rootfs has pre-installed the vAccel libraries, the example application the images and models. Additionally, it installed for us the following Firecracker configuration for booting the VM. In similar fashion as before, we launch the VM: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Launch the Firecracker VM ./bin/firecracker --api-sock fc.sock --config-file share/config_virtio_accel.json --seccomp-level 0 This will launch the Firecracker VM and it will result in a login prompt in the VM, to which we can login using the root user without a password. Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0 vaccel-guest login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Fri Feb 5 17:32:12 UTC 2021 on ttyS0 root@vaccel-guest:~# Once, in the prompt of the guest we setup the environment and run the example: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1 Note : In this case, we use the VirtIO plugin since we are inside the VM and we do not need to define the path to the pre-trained model. The latter is necessary only on the host where we use the jetson plugin. The result of the classification should be the same as before: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute","title":"Running in a Firecracker VM"},{"location":"quickstart/#docker-execution","text":"If you don't have a Jetson-inference installation available on your machine, you can use our nubificus/vaccel-deps Docker image to run a vAccel application. You need to install the Nvidia container runtime following the instructions here , which allows you to use NVIDIA GPU devices inside Docker containers. Once, you're all setup with the nvidia runtime you can run the application: docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/classify share/images/dog_0.jpg 1 Similarly, we can launch a Firecracker VM inside the container. In this case, we need to add the --privileged flag to allow launching VMs inside the container and -it to be able to use the VM's console. docker run --runtime=nvidia --rm --gpus all -v $(pwd):$(pwd) -w $(pwd) \\ --privileged \\ -it \\ -e LD_LIBRARY_PATH=$(pwd)/lib \\ -e VACCEL_BACKENDS=./lib/libvaccel-jetson.so \\ -e VACCEL_IMAGENET_NETWORKS=$(pwd)/share/networks \\ nubificus/vaccel-deps:latest \\ ./bin/firecracker --api-sock fc.sock --config-file ./share/config_virtio_accel.json --seccomp-level 0 which we'll give us a console inside the VM, from which we can run our application the same way as we did before: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1","title":"Docker execution"},{"location":"run/","text":"Running a simple example Building a vaccel application We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images Running the example Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: $ ls classify images networks $ VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running a simple example"},{"location":"run/#running-a-simple-example","text":"","title":"Running a simple example"},{"location":"run/#building-a-vaccel-application","text":"We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images","title":"Building a vaccel application"},{"location":"run/#running-the-example","text":"Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: $ ls classify images networks $ VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running the example"},{"location":"vaccelrt/","text":"Building the vAccel Runtime vAccelRT vAccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework. Build & Installation 1. Cloning and preparing the build directory ~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb 2. Building the core runtime system # This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install 3. Building the plugins Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin. VirtIO plugin building options Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable. Jetson plugin building options The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Building the vAccel Runtime"},{"location":"vaccelrt/#building-the-vaccel-runtime","text":"","title":"Building the vAccel Runtime"},{"location":"vaccelrt/#vaccelrt","text":"vAccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework.","title":"vAccelRT"},{"location":"vaccelrt/#build-installation","text":"","title":"Build &amp; Installation"},{"location":"vaccelrt/#1-cloning-and-preparing-the-build-directory","text":"~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb","title":"1. Cloning and preparing the build directory"},{"location":"vaccelrt/#2-building-the-core-runtime-system","text":"# This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install","title":"2. Building the core runtime system"},{"location":"vaccelrt/#3-building-the-plugins","text":"Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin.","title":"3. Building the plugins"},{"location":"vaccelrt/#virtio-plugin-building-options","text":"Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable.","title":"VirtIO plugin building options"},{"location":"vaccelrt/#jetson-plugin-building-options","text":"The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Jetson plugin building options"},{"location":"virtio/","text":"Virtio Accel Virtio Accel was inspired by the Virtio Crypto paravirtual device for enabling accelerated crypto operations inside a Virtual Machine. We extended the idea of accelerated crypto operations to any acceleratable operation, which resulted in the virtio-accel module and the corresponding vAccelRT plugin that uses it to allow vAccel applications to use accelerated operations from within a Virtual Machine. Currently we support the AWS Firecracker and QEMU hypervisors. Building the kernel module We first need to build the virtio-accel module that will be used inside our VM. We will need the source of the kernel module and a kernel tree. Firecracker For Firecracker we use Linux kernel version 4.20 , because that is consistent with the configuration the AWS Firecracker team is shipping but the module itself should be able to build with newer kernel versions, as well. For example, for our k8s deployment of vAccel we use Linux Kernel version 5.4.60 Let's fetch and build the kernel # Fetch the Linux kernel tree git clone --depth=1 -b v4.20 https://github.com/torvalds/linux.git cd linux # Fetch the Firecracker config wget https://raw.githubusercontent.com/firecracker-microvm/firecracker/master/resources/microvm-kernel-x86_64.config -O arch/x86/configs/microvm.config touch .config make microvm.config make vmlinux cd .. ls linux/vmlinux linux/vmlinux You should now have a newly built kernel image under linux/vmlinux , which you can use to boot with your Firecracker VM. And now we can build the module: git clone https://github.com/cloudkernels/virtio-accel make -C virtio-accel KDIR=$(pwd)/linux ZC=0 ls virtio-accel/virtio_accel.ko virtio-accel/virtio_accel.ko This should build the module under virtio-accel/virtio_accel.ko . Note : The virtio-accel module supports zero-copy operations. This functionality has not been implemented yet in Firecracker, so we disabled the feature passing ZC=0 to the Makefile . QEMU TBD Building the vAccelRT plugin The vAccelRT runtime ships with a virtio plugin which speaks the virtio-accel module's ioctl language to offload computation from a VM guest to the host. # Fetch the vAccelRT repo git clone --recursive https://github.com/cloudkernels/vaccelrt cd vaccelrt mkdir build cd build cmake .. -DBUILD_PLUGIN_VIRTIO=ON -DVIRTIO_ACCEL_ROOT=../../virtio-accel make cd ../../ ls vaccelrt/build/plugins/virtio/libvaccel-virtio.so vaccelrt/build/plugins/virtio/libvaccel-virtio.so This builds the plugin and places it under vaccelrt/build/plugins/virtio/libvaccel-virtio.so . You can now insert the plugin to your VMs rootfs and run a vAccel application.","title":"Virtio Accel"},{"location":"virtio/#virtio-accel","text":"Virtio Accel was inspired by the Virtio Crypto paravirtual device for enabling accelerated crypto operations inside a Virtual Machine. We extended the idea of accelerated crypto operations to any acceleratable operation, which resulted in the virtio-accel module and the corresponding vAccelRT plugin that uses it to allow vAccel applications to use accelerated operations from within a Virtual Machine. Currently we support the AWS Firecracker and QEMU hypervisors.","title":"Virtio Accel"},{"location":"virtio/#building-the-kernel-module","text":"We first need to build the virtio-accel module that will be used inside our VM. We will need the source of the kernel module and a kernel tree.","title":"Building the kernel module"},{"location":"virtio/#firecracker","text":"For Firecracker we use Linux kernel version 4.20 , because that is consistent with the configuration the AWS Firecracker team is shipping but the module itself should be able to build with newer kernel versions, as well. For example, for our k8s deployment of vAccel we use Linux Kernel version 5.4.60 Let's fetch and build the kernel # Fetch the Linux kernel tree git clone --depth=1 -b v4.20 https://github.com/torvalds/linux.git cd linux # Fetch the Firecracker config wget https://raw.githubusercontent.com/firecracker-microvm/firecracker/master/resources/microvm-kernel-x86_64.config -O arch/x86/configs/microvm.config touch .config make microvm.config make vmlinux cd .. ls linux/vmlinux linux/vmlinux You should now have a newly built kernel image under linux/vmlinux , which you can use to boot with your Firecracker VM. And now we can build the module: git clone https://github.com/cloudkernels/virtio-accel make -C virtio-accel KDIR=$(pwd)/linux ZC=0 ls virtio-accel/virtio_accel.ko virtio-accel/virtio_accel.ko This should build the module under virtio-accel/virtio_accel.ko . Note : The virtio-accel module supports zero-copy operations. This functionality has not been implemented yet in Firecracker, so we disabled the feature passing ZC=0 to the Makefile .","title":"Firecracker"},{"location":"virtio/#qemu","text":"TBD","title":"QEMU"},{"location":"virtio/#building-the-vaccelrt-plugin","text":"The vAccelRT runtime ships with a virtio plugin which speaks the virtio-accel module's ioctl language to offload computation from a VM guest to the host. # Fetch the vAccelRT repo git clone --recursive https://github.com/cloudkernels/vaccelrt cd vaccelrt mkdir build cd build cmake .. -DBUILD_PLUGIN_VIRTIO=ON -DVIRTIO_ACCEL_ROOT=../../virtio-accel make cd ../../ ls vaccelrt/build/plugins/virtio/libvaccel-virtio.so vaccelrt/build/plugins/virtio/libvaccel-virtio.so This builds the plugin and places it under vaccelrt/build/plugins/virtio/libvaccel-virtio.so . You can now insert the plugin to your VMs rootfs and run a vAccel application.","title":"Building the vAccelRT plugin"},{"location":"k8s/kata/","text":"vAccel on k8s using Kata-containers & Firecracker Prerequisites In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter nvidia GPU which supports CUDA ( for now ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries) Quick start Deploy vAccel with Kata We rely on kata-containers/kata-deploy to create the vaccel-kata-deploy daemon. Our fork repo can be found on cloudkernels/packaging . We are working on building a Kata Containers release with vAccel support. Label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Create service account and cluster role for the kata-deploy daemon $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-rbac/base/kata-rbac.yaml Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m ... ... Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl -n kube-system logs kata-deploy-575tm ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before (~2GB). $ kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full?ref=vaccel-dev Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml Deploy an image classification function as a Service The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl create -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/examples/web-classify.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ k3s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m Run a curl command (from a cluster node) like the following to send your POST request to the Service web-classify-kata-fc (to access the Service from outside the cluster use nodePort or deploy an ingress route) $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST 10.43.214.52:80/classify --data-binary @- And see the result of the image classification! --2021-02-05 20:17:15-- https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg Resolving pbs.twimg.com (pbs.twimg.com)... 2606:2800:134:fa2:1627:1fe:edb:1665, 192.229.233.50 Connecting to pbs.twimg.com (pbs.twimg.com)|2606:2800:134:fa2:1627:1fe:edb:1665|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 12605 (12K) [image/jpeg] Saving to: 'STDOUT' - 100%[================================================>] 12.31K --.-KB/s in 0.04s 2021-02-05 20:17:15 (296 KB/s) - written to stdout [12605/12605] [\"web-classify-kata-fc-567bddccc4-s79b5\"]: \"29.761% wall clock\"","title":"vAccel on k8s using Kata-containers & Firecracker"},{"location":"k8s/kata/#vaccel-on-k8s-using-kata-containers-firecracker","text":"","title":"vAccel on k8s using Kata-containers &amp; Firecracker"},{"location":"k8s/kata/#prerequisites","text":"In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter nvidia GPU which supports CUDA ( for now ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries)","title":"Prerequisites"},{"location":"k8s/kata/#quick-start","text":"","title":"Quick start"},{"location":"k8s/kata/#deploy-vaccel-with-kata","text":"We rely on kata-containers/kata-deploy to create the vaccel-kata-deploy daemon. Our fork repo can be found on cloudkernels/packaging . We are working on building a Kata Containers release with vAccel support. Label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Create service account and cluster role for the kata-deploy daemon $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-rbac/base/kata-rbac.yaml Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m ... ... Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl -n kube-system logs kata-deploy-575tm ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before (~2GB). $ kubectl apply -k github.com/cloudkernels/packaging/kata-deploy/kata-deploy/overlays/full?ref=vaccel-dev Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml","title":"Deploy vAccel with Kata"},{"location":"k8s/kata/#deploy-an-image-classification-function-as-a-service","text":"The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl create -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/examples/web-classify.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ k3s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m Run a curl command (from a cluster node) like the following to send your POST request to the Service web-classify-kata-fc (to access the Service from outside the cluster use nodePort or deploy an ingress route) $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST 10.43.214.52:80/classify --data-binary @- And see the result of the image classification! --2021-02-05 20:17:15-- https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg Resolving pbs.twimg.com (pbs.twimg.com)... 2606:2800:134:fa2:1627:1fe:edb:1665, 192.229.233.50 Connecting to pbs.twimg.com (pbs.twimg.com)|2606:2800:134:fa2:1627:1fe:edb:1665|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 12605 (12K) [image/jpeg] Saving to: 'STDOUT' - 100%[================================================>] 12.31K --.-KB/s in 0.04s 2021-02-05 20:17:15 (296 KB/s) - written to stdout [12605/12605] [\"web-classify-kata-fc-567bddccc4-s79b5\"]: \"29.761% wall clock\"","title":"Deploy an image classification function as a Service"},{"location":"unikernels/unikraft/","text":"To run an image classification app in Unikraft using vAccel, we need: an NVIDIA GPU which supports CUDA the appropriate device drivers jetson-inference As long as we have the above depedencies, we can proceed setting up the vAccel environment for our example. A comprehensive walk through on installing the above dependencies is given in Jetson-inference . Additionally, we will need to set up 3 components: The vAccel Runtime system, vAccelrt on the bost A hypervisor with vAccel support, in our case QEMU and of course Unikraft To make things easier we have created some scripts and Docketfiles which produce a vAccel enabled QEMU and a Unikraft unikernel. You can get them from this repo. The first 2 components can be built using the build.sh script with the -u option, like: bash build.sh -u The last component can be created using the build_guest.sh script with the -u option, like: bash build_guest.sh -u After succefully building all components we can just fire up our unikernel using the command: bash run.sh This command will start Unikraft over QEMU classifying the image dog_0.jpg under guest/data/ directory of this repo. Let's take a closer look on what is happening inside the containers and how we can perform each step by ourselves. Setting up vAccert for the host vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case the backend plugin will be an NVIDIA GPU, and more specifically, jetson-inference, using TensorRT. As a result we need to build the Jetson plugin for vAccelrt. At first let's get the source code: git clone --recursive https://github.com/cloudkernels/vaccelrt.git cd vaccelrt As we mentioned before, we will also build the jetson plugin and we will specify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so Build QEMU with vAccel support For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. First lets get the source code: git clone --recursive https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side! Build the Unikraft application We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but our fork. In our fork we have added an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under the build directory. Fire it up! After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run our unikernel using the command below: LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"How to run a vAccel application using Unikraft"},{"location":"unikernels/unikraft/#setting-up-vaccert-for-the-host","text":"vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case the backend plugin will be an NVIDIA GPU, and more specifically, jetson-inference, using TensorRT. As a result we need to build the Jetson plugin for vAccelrt. At first let's get the source code: git clone --recursive https://github.com/cloudkernels/vaccelrt.git cd vaccelrt As we mentioned before, we will also build the jetson plugin and we will specify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so","title":"Setting up vAccert for the host"},{"location":"unikernels/unikraft/#build-qemu-with-vaccel-support","text":"For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. First lets get the source code: git clone --recursive https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side!","title":"Build QEMU with vAccel support"},{"location":"unikernels/unikraft/#build-the-unikraft-application","text":"We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but our fork. In our fork we have added an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under the build directory.","title":"Build the Unikraft application"},{"location":"unikernels/unikraft/#fire-it-up","text":"After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run our unikernel using the command below: LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"Fire it up!"}]}
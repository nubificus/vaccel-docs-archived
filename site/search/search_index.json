{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to vAccel vAccel is a framework offering hardware acceleration primitive with focus on protability. It exposes to programmers \"acceleratable\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale. vAccel design","title":"Home"},{"location":"#welcome-to-vaccel","text":"vAccel is a framework offering hardware acceleration primitive with focus on protability. It exposes to programmers \"acceleratable\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale.","title":"Welcome to vAccel"},{"location":"#vaccel-design","text":"","title":"vAccel design"},{"location":"build/","text":"Building the vAccel stack vaccel Runtime System vaccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework. VirtIO backend TODO: put link to VirtIO README.md Jetson Inference backend Build & Installation 1. Cloning and preparing the build directory ~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb 2. Building the core runtime system # This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install 3. Building the plugins Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin. VirtIO plugin building options Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable. Jetson plugin building options The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Building the vAccel stack"},{"location":"build/#building-the-vaccel-stack","text":"","title":"Building the vAccel stack"},{"location":"build/#vaccel-runtime-system","text":"vaccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework.","title":"vaccel Runtime System"},{"location":"build/#virtio-backend","text":"TODO: put link to VirtIO README.md","title":"VirtIO backend"},{"location":"build/#jetson-inference-backend","text":"","title":"Jetson Inference backend"},{"location":"build/#build-installation","text":"","title":"Build &amp; Installation"},{"location":"build/#1-cloning-and-preparing-the-build-directory","text":"~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb","title":"1. Cloning and preparing the build directory"},{"location":"build/#2-building-the-core-runtime-system","text":"# This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install","title":"2. Building the core runtime system"},{"location":"build/#3-building-the-plugins","text":"Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin.","title":"3. Building the plugins"},{"location":"build/#virtio-plugin-building-options","text":"Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable.","title":"VirtIO plugin building options"},{"location":"build/#jetson-plugin-building-options","text":"The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Jetson plugin building options"},{"location":"requirements/","text":"System Requirements vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload, we will use a set of NVIDIA 1 GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. Our common vAccel plugin is based on jetson-inference a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install At the time of this writing, support is available for NVIDIA (x86 & aarch64) as well as Intel Iris integrated GPUs and Google Coral Edge TPUs. \u21a9","title":"System Requirements"},{"location":"requirements/#system-requirements","text":"vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload, we will use a set of NVIDIA 1 GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. Our common vAccel plugin is based on jetson-inference a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install At the time of this writing, support is available for NVIDIA (x86 & aarch64) as well as Intel Iris integrated GPUs and Google Coral Edge TPUs. \u21a9","title":"System Requirements"},{"location":"run/","text":"Running a simple example Building a vaccel application We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images Running the example Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: $ ls classify images networks $ VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running a simple example"},{"location":"run/#running-a-simple-example","text":"","title":"Running a simple example"},{"location":"run/#building-a-vaccel-application","text":"We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images","title":"Building a vaccel application"},{"location":"run/#running-the-example","text":"Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: $ ls classify images networks $ VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running the example"},{"location":"k8s/kata/","text":"vAccel on k8s using Kata & Firecracker TODO - add proper links to yaml files - add correct outputs - add components build documentation Prerequisites In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter NVIDIA GPU which supports CUDA ( for now ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries) Quick start We built on kata-containers/kata-deploy to deploy vAccel on Kata Containers. Our fork repo can be found on cloudkernels/packaging Deploy vAccel with Kata First label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m default web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 76m default web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 76m ... ... $ k3s kubectl -n kube-system logs kata-deploy-575tm ... ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel **warning: containerd-shim-kata-fc-v2 already exists** node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before (~2GB). $ kubectl kata-deploy-full.yaml Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml Deploy an image classification function as a Service The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl web-classify.yaml NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ k3s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST classify.nbfc.io/classify --data-binary @-","title":"vAccel on k8s using Kata & Firecracker"},{"location":"k8s/kata/#vaccel-on-k8s-using-kata-firecracker","text":"TODO - add proper links to yaml files - add correct outputs - add components build documentation","title":"vAccel on k8s using Kata &amp; Firecracker"},{"location":"k8s/kata/#prerequisites","text":"In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter NVIDIA GPU which supports CUDA ( for now ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries)","title":"Prerequisites"},{"location":"k8s/kata/#quick-start","text":"We built on kata-containers/kata-deploy to deploy vAccel on Kata Containers. Our fork repo can be found on cloudkernels/packaging","title":"Quick start"},{"location":"k8s/kata/#deploy-vaccel-with-kata","text":"First label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m default web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 76m default web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 76m ... ... $ k3s kubectl -n kube-system logs kata-deploy-575tm ... ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel **warning: containerd-shim-kata-fc-v2 already exists** node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before (~2GB). $ kubectl kata-deploy-full.yaml Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml","title":"Deploy vAccel with Kata"},{"location":"k8s/kata/#deploy-an-image-classification-function-as-a-service","text":"The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl web-classify.yaml NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ k3s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST classify.nbfc.io/classify --data-binary @-","title":"Deploy an image classification function as a Service"},{"location":"unikernels/unikraft/","text":"In this post we will describe the process of running an image classification app in Unikraft using vAccel. In order to do that we gonna need; a NVIDIA GPU which supports CUDA the appropriate drivers and jetson-inference As long as we have the above depedencies, we can move on setting up the vAccel environment for our example. We will need to set up 3 components: vAccelrt on the bost A hypervisor with vAccel suuport, in our case QEMU and of course Unikraft To make things easier we have created some scripts and Docketfiles which produce a vAccel enabled QEMU and a Unikraft unikernel. You can get them from this repo. The first 2 components can be built using the build.sh script with the -u option, like: bash build.sh -u The last component can be created using the build_guest.sh script with the -u option, like: bash build_guest.sh -u After succefully building all components we can just fire up our unikernel using the command: bash run.sh This command will start Unikraft over QEMU classifying the image dog_0.jpg under guest/data/ directory of this repo. Let's take a closer look on what is happening inside the containers and how we can do each step by ourselves. Setting up vAccert for the host vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case this hardware will be a NVIDIA GPU. As a result we need to build the Jetson plugin for vAccelrt too. At first let's get the source code git clone https://github.com/cloudkernels/vaccelrt.git cd vaccelrt git submodule update --init As we said before we will also build the jetson plugin and we will speccify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so Build QEMU with vAccel support As always at first we need to get the source code. For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. git clone https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel git submodule update --init We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side. Build the Unikraft application We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but out fork. In out fork there is also an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages.. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under build direcotry. Fire it up After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run out unikernel. LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"How to run a vAccel application using Unikraft"},{"location":"unikernels/unikraft/#setting-up-vaccert-for-the-host","text":"vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case this hardware will be a NVIDIA GPU. As a result we need to build the Jetson plugin for vAccelrt too. At first let's get the source code git clone https://github.com/cloudkernels/vaccelrt.git cd vaccelrt git submodule update --init As we said before we will also build the jetson plugin and we will speccify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so","title":"Setting up vAccert for the host"},{"location":"unikernels/unikraft/#build-qemu-with-vaccel-support","text":"As always at first we need to get the source code. For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. git clone https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel git submodule update --init We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side.","title":"Build QEMU with vAccel support"},{"location":"unikernels/unikraft/#build-the-unikraft-application","text":"We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but out fork. In out fork there is also an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages.. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under build direcotry.","title":"Build the Unikraft application"},{"location":"unikernels/unikraft/#fire-it-up","text":"After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run out unikernel. LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"Fire it up"}]}
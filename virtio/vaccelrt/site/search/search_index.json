{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to vAccel vAccel is a framework offering hardware acceleration primitive with focus on protability. It exposes to programmers \"acceleratable\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. QEMU and AWS Firecracker VMMs are currently supported compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale. vAccel design The core component of vAccel is the vaccel runtime (vaccelrt). vaccelrt is designed in a modular way. The core runtime exposes the vAccel API to user applications and dispatches requests to one of many backend plugins , which are the components that implement the vAccel API on a particular hardware accelerator. vAccel runtime stack The user application links against the core runtime","title":"Home"},{"location":"#welcome-to-vaccel","text":"vAccel is a framework offering hardware acceleration primitive with focus on protability. It exposes to programmers \"acceleratable\" functions and abstracts away hardware complexity by means of a pluggable design. The design goals of vAccel are: portability : vAccel applications can be deployed in machines with different hardware accelerators without re-writing or re-compilation. security : A vAccel application can be deployed, as is , in a VM to ensure isolation in multi-tenant environments. QEMU and AWS Firecracker VMMs are currently supported compatibility : vAccel supports the OCI container format through integration with the Kata containers framework. low-overhead : vAccel uses a very efficient transport layer for offloading acceleratable functions from insde the VM to the host, incuring minimum overhead. scalability : Integration with k8s allows deployment of vAccel applications at scale.","title":"Welcome to vAccel"},{"location":"#vaccel-design","text":"The core component of vAccel is the vaccel runtime (vaccelrt). vaccelrt is designed in a modular way. The core runtime exposes the vAccel API to user applications and dispatches requests to one of many backend plugins , which are the components that implement the vAccel API on a particular hardware accelerator. vAccel runtime stack The user application links against the core runtime","title":"vAccel design"},{"location":"build/","text":"Building the vAccel stack vaccel Runtime System vaccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework. VirtIO backend TODO: put link to VirtIO README.md Jetson Inference backend Build & Installation 1. Cloning and preparing the build directory ~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb 2. Building the core runtime system # This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install 3. Building the plugins Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin. VirtIO plugin building options Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable. Jetson plugin building options The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Building the vAccel stack"},{"location":"build/#building-the-vaccel-stack","text":"","title":"Building the vAccel stack"},{"location":"build/#vaccel-runtime-system","text":"vaccel is a runtime system for hardware acceleration. It provides an API with a set of functions that the runtime is able to offload to hardware acceleration devices. The design of the runtime is modular, it consists of a front-end library which exposes the API to the user application and a set of back-end plugins that are responsible to offload computations to the accelerator. This design decouples the user application from the actual accelerator specific code. The advantage of this choice is that the application can make use of different hardware accelerators without extra development cost or re-compiling. This repo includes the core runtime system, and back-end plugins for VirtIO and the Jetson Inference framework.","title":"vaccel Runtime System"},{"location":"build/#virtio-backend","text":"TODO: put link to VirtIO README.md","title":"VirtIO backend"},{"location":"build/#jetson-inference-backend","text":"","title":"Jetson Inference backend"},{"location":"build/#build-installation","text":"","title":"Build &amp; Installation"},{"location":"build/#1-cloning-and-preparing-the-build-directory","text":"~ \u00bb git clone https://github.com/cloudkernels/vaccelrt.git Cloning into 'vaccelrt'... remote: Enumerating objects: 215, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (130/130), done. remote: Total 215 (delta 115), reused 173 (delta 82), pack-reused 0 Receiving objects: 100% (215/215), 101.37 KiB | 804.00 KiB/s, done. Resolving deltas: 100% (115/115), done. ~ \u00bb cd vaccelrt ~/vaccelrt(master) \u00bb mkdir build ~/vaccelrt(master) \u00bb cd build ~/vaccelrt/build(master) \u00bb","title":"1. Cloning and preparing the build directory"},{"location":"build/#2-building-the-core-runtime-system","text":"# This sets the installation path to ${HOME}/.local ~/vaccelrt/build(master) \u00bb cmake -DCMAKE_INSTALL_PREFIX=~/.local .. ~/vaccelrt/build(master) \u00bb mak ~/vaccelrt/build(master) \u00bb mak install","title":"2. Building the core runtime system"},{"location":"build/#3-building-the-plugins","text":"Building the plugins is disabled, by default. You can enable building one or more plugins at configuration time of CMake by setting the corresponding variable of the following table to ON Backend Plugin Variable Default virtio BUILD_PLUGIN_VIRTIO OFF jetson BUILD_PLUGIN_JETSON OFF For example: cmake -DBUILD_PLUGIN_VIRTIO=ON .. will enable building the virtio backend plugin.","title":"3. Building the plugins"},{"location":"build/#virtio-plugin-building-options","text":"Variable Values Default VIRTIO_ACCEL_ROOT Path to virtio module installation /usr/local/include The VirtIO plugin uses the virtio-accel kernel module to offload requests to the host. When building we need to point CMake to the location of virtio-accel installation prefix using the VIRTIO_ACCEL_ROOT variable.","title":"VirtIO plugin building options"},{"location":"build/#jetson-plugin-building-options","text":"The jetson inference backends depends on the Jetson inference framework, a corresponding CUDA installation and the STB library. Variable Values Default CUDA_DIR Path to CUDA installation /usr/local/cuda/targets/x86_64-linux JETSON_DIR Path to Jetson installation /usr/local STB_DIR Path to STB installation /usr/local","title":"Jetson plugin building options"},{"location":"coral/","text":"Google Coral TPU To build the vAccel Google Coral TPU plugin, we will use a simple image classification example based on tflite To execute an image classification operation on the Google Coral TPU, we first need to implement the operation using one of the available acceleration frameworks and then register this function to the vAccel plugin we are building. To achieve this, we patch a tflite example for Google coral, and build the available image classification function as a shared library, exposing the following API to the relevant plugin: int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) Patch the code The first step in order to build the library is to get the example code from the Google Coral github repo: git clone https://github.com/google-coral/tflite Then we need to apply the following patch ( libify_classify.patch ) to the relevant example: diff --git a/cpp/examples/classification/classify.cc b/cpp/examples/classification/classify.cc index 69dfe04..1129976 100644 --- a/cpp/examples/classification/classify.cc +++ b/cpp/examples/classification/classify.cc @@ -22,6 +22,72 @@ int32_t ToInt32(const char p[4]) { return (p[3] << 24) | (p[2] << 16) | (p[1] << 8) | p[0]; } +std::vector<uint8_t> ReadBmpImageFromBuf(char* buf, + int* out_width = nullptr, + int* out_height = nullptr, + int* out_channels = nullptr) { + char header[kBmpHeaderSize]; + memcpy(header, buf, sizeof(header)); + + const char* file_header = header; + const char* info_header = header + kBmpFileHeaderSize; + char *ptr = buf + sizeof(header); + + if (file_header[0] != 'B' || file_header[1] != 'M') + return {}; // Invalid file type. + + const int channels = info_header[14] / 8; + if (channels != 1 && channels != 3) return {}; // Unsupported bits per pixel. + + if (ToInt32(&info_header[16]) != 0) return {}; // Unsupported compression. + + uint32_t offset = ToInt32(&file_header[10]); +#if 0 + if (offset > kBmpHeaderSize)// && + //!file.seekg(offset - kBmpHeaderSize, std::ios::cur)) + return {}; // Seek failed. +#endif + + offset -= kBmpHeaderSize; + //printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + //printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + //printf(\"offset:%d\\n\", offset); + ptr = ptr+offset; + int width = ToInt32(&info_header[4]); + if (width < 0) return {}; // Invalid width. + + int height = ToInt32(&info_header[8]); + const bool top_down = height < 0; + if (top_down) height = -height; + + const int line_bytes = width * channels; + const int line_padding_bytes = + 4 * ((8 * channels * width + 31) / 32) - line_bytes; + std::vector<uint8_t> image(line_bytes * height); + for (int i = 0; i < height; ++i) { + uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + memcpy(line, ptr, line_bytes); +#if 0 + if (!file.read(reinterpret_cast<char*>(line), line_bytes)) + return {}; // Read failed. + if (!file.seekg(line_padding_bytes, std::ios::cur)) + return {}; // Seek failed. +#endif + //printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); + ptr = ptr + line_padding_bytes; + ptr = ptr + line_bytes; + if (channels == 3) { + for (int j = 0; j < width; ++j) std::swap(line[3 * j], line[3 * j + 2]); + } + } + + if (out_width) *out_width = width; + if (out_height) *out_height = height; + if (out_channels) *out_channels = channels; + return image; +} + + std::vector<uint8_t> ReadBmpImage(const char* filename, int* out_width = nullptr, int* out_height = nullptr, @@ -50,6 +116,9 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, !file.seekg(offset - kBmpHeaderSize, std::ios::cur)) return {}; // Seek failed. + printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + printf(\"offset:%d\\n\", offset); int width = ToInt32(&info_header[4]); if (width < 0) return {}; // Invalid width. @@ -63,6 +132,7 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, std::vector<uint8_t> image(line_bytes * height); for (int i = 0; i < height; ++i) { uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); if (!file.read(reinterpret_cast<char*>(line), line_bytes)) return {}; // Read failed. if (!file.seekg(line_padding_bytes, std::ios::cur)) @@ -117,18 +187,12 @@ std::vector<std::pair<int, float>> Sort(const std::vector<float>& scores, } } // namespace -int main(int argc, char* argv[]) { - if (argc != 5) { - std::cerr << argv[0] - << \" <model_file> <label_file> <image_file> <threshold>\" - << std::endl; - return 1; - } - - const std::string model_file = argv[1]; - const std::string label_file = argv[2]; - const std::string image_file = argv[3]; - const float threshold = std::stof(argv[4]); +extern \"C\" int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) +{ + const std::string model_file (model_file_ptr); + const std::string label_file (label_file_ptr); + const std::string image_file (image_file_ptr); + const float threshold = thres; // Find TPU device. size_t num_devices; @@ -151,7 +215,8 @@ int main(int argc, char* argv[]) { // Load image. int image_bpp, image_width, image_height; auto image = - ReadBmpImage(image_file.c_str(), &image_width, &image_height, &image_bpp); + ReadBmpImageFromBuf(image_file_ptr, &image_width, &image_height, &image_bpp); + //ReadBmpImage(image_file_ptr, &image_width, &image_height, &image_bpp); if (image.empty()) { std::cerr << \"Cannot read image from \" << image_file << std::endl; return 1; @@ -174,7 +239,7 @@ int main(int argc, char* argv[]) { auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0); - interpreter->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate}); + interpreter->ModifyGraphWithDelegate(delegate); // Allocate tensors. if (interpreter->AllocateTensors() != kTfLiteOk) { @@ -204,9 +269,13 @@ int main(int argc, char* argv[]) { // Get interpreter output. auto results = Sort(Dequantize(*interpreter->output_tensor(0)), threshold); + *tags = strdup(GetLabel(labels, results[0].first).c_str()); + *out_len = strlen(*tags); +#if 0 for (auto& result : results) std::cout << std::setw(7) << std::fixed << std::setprecision(5) << result.second << GetLabel(labels, result.first) << std::endl; +#endif return 0; } cd tflite patch -p1 < ../libify_classify.path Alternatively you could just clone from our forked repo: git clone https://github.com/nubificus/tflite -b vaccel Then, we need to install the requirements & build tflite for google coral (could take some time, so please be patient ;-)). Download the latest Edge TPU runtime You can grab the latest Edge TPU runtime archive from https://coral.ai/software/ . Then extract it next to the Makefile: cd tflite/cpp/examples/classification/ wget https://dl.google.com/coral/edgetpu_api/edgetpu_runtime_20200710.zip Download and build Tensorflow git clone https://github.com/tensorflow/tensorflow.git tensorflow/tensorflow/lite/tools/make/download_dependencies.sh tensorflow/tensorflow/lite/tools/make/build_lib.sh Build the shared library Finally, build the classify shared object that we will then link to the vAccel Google Coral TPU plugin. g++ -shared -fPIC -std=c++11 classify.cc -o /usr/local/lib/libclassify.so -I/data/tflite/cpp/examples/classification/edgetpu_runtime/libedgetpu/ -Itensorflow/ -Itensorflow//tensorflow/lite/tools/make/downloads/flatbuffers/include -Ltensorflow//tensorflow/lite/tools/make/gen/generic-aarch64_armv8-a/lib -Ledgetpu_runtime/libedgetpu/direct/aarch64/ -l:libedgetpu.so.1.0 -lpthread -lm -ldl tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a Build vAccelRT plugin Following the generic process for vAccelRT building (from Building vAccelRT ) we now have to enable the vAccel Google Coral Edge TPU plugin. In short, the previous steps for vAccelRT were the following 1 : git clone --recursive https://github.com/cloudkernels/vaccelrt -b feat_gcoral_plugin cd vaccelrt mkdir build cd build cmake ../ The additional step now is to enable the plugin via cmake: cd vaccelrt mkdir build_coral && cd build_coral cmake ../ -DBUILD_PLUGIN_CORAL=ON The output of the above command should be something like the following: -- The C compiler identification is GNU 8.3.0 -- The CXX compiler identification is GNU 8.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral/googletest-download Scanning dependencies of target googletest [ 11%] Creating directories for 'googletest' [ 22%] Performing download step (git clone) for 'googletest' Cloning into 'googletest-src'... Note: checking out 'release-1.8.1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at 2fe3bd99 Merge pull request #1433 from dsacre/fix-clang-warnings [ 33%] No patch step for 'googletest' [ 44%] Performing update step for 'googletest' [ 55%] No configure step for 'googletest' [ 66%] No build step for 'googletest' [ 77%] No install step for 'googletest' [ 88%] No test step for 'googletest' [100%] Completed 'googletest' [100%] Built target googletest -- Found PythonInterp: /usr/bin/python (found version \"2.7.16\") -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Found rt: /usr/lib/aarch64-linux-gnu/librt.so Include directories: /data/vaccelrt/src -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral Finally, building the plugin is as simple as issueing a make command: make The plugin is available at: vaccelrt/build_coral/plugins/coral/libvaccel-coral.so Extra Files To run an example on the Google Coral Edge TPU, follow the instructions available at Running a simple example . The files needed for classification are: mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt available for download from: https://github.com/google-coral/test_data . Support for Google Coral is WiP , and will be available shortly in the main branch. \u21a9","title":"Google Coral TPU"},{"location":"coral/#google-coral-tpu","text":"To build the vAccel Google Coral TPU plugin, we will use a simple image classification example based on tflite To execute an image classification operation on the Google Coral TPU, we first need to implement the operation using one of the available acceleration frameworks and then register this function to the vAccel plugin we are building. To achieve this, we patch a tflite example for Google coral, and build the available image classification function as a shared library, exposing the following API to the relevant plugin: int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len)","title":"Google Coral TPU"},{"location":"coral/#patch-the-code","text":"The first step in order to build the library is to get the example code from the Google Coral github repo: git clone https://github.com/google-coral/tflite Then we need to apply the following patch ( libify_classify.patch ) to the relevant example: diff --git a/cpp/examples/classification/classify.cc b/cpp/examples/classification/classify.cc index 69dfe04..1129976 100644 --- a/cpp/examples/classification/classify.cc +++ b/cpp/examples/classification/classify.cc @@ -22,6 +22,72 @@ int32_t ToInt32(const char p[4]) { return (p[3] << 24) | (p[2] << 16) | (p[1] << 8) | p[0]; } +std::vector<uint8_t> ReadBmpImageFromBuf(char* buf, + int* out_width = nullptr, + int* out_height = nullptr, + int* out_channels = nullptr) { + char header[kBmpHeaderSize]; + memcpy(header, buf, sizeof(header)); + + const char* file_header = header; + const char* info_header = header + kBmpFileHeaderSize; + char *ptr = buf + sizeof(header); + + if (file_header[0] != 'B' || file_header[1] != 'M') + return {}; // Invalid file type. + + const int channels = info_header[14] / 8; + if (channels != 1 && channels != 3) return {}; // Unsupported bits per pixel. + + if (ToInt32(&info_header[16]) != 0) return {}; // Unsupported compression. + + uint32_t offset = ToInt32(&file_header[10]); +#if 0 + if (offset > kBmpHeaderSize)// && + //!file.seekg(offset - kBmpHeaderSize, std::ios::cur)) + return {}; // Seek failed. +#endif + + offset -= kBmpHeaderSize; + //printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + //printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + //printf(\"offset:%d\\n\", offset); + ptr = ptr+offset; + int width = ToInt32(&info_header[4]); + if (width < 0) return {}; // Invalid width. + + int height = ToInt32(&info_header[8]); + const bool top_down = height < 0; + if (top_down) height = -height; + + const int line_bytes = width * channels; + const int line_padding_bytes = + 4 * ((8 * channels * width + 31) / 32) - line_bytes; + std::vector<uint8_t> image(line_bytes * height); + for (int i = 0; i < height; ++i) { + uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + memcpy(line, ptr, line_bytes); +#if 0 + if (!file.read(reinterpret_cast<char*>(line), line_bytes)) + return {}; // Read failed. + if (!file.seekg(line_padding_bytes, std::ios::cur)) + return {}; // Seek failed. +#endif + //printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); + ptr = ptr + line_padding_bytes; + ptr = ptr + line_bytes; + if (channels == 3) { + for (int j = 0; j < width; ++j) std::swap(line[3 * j], line[3 * j + 2]); + } + } + + if (out_width) *out_width = width; + if (out_height) *out_height = height; + if (out_channels) *out_channels = channels; + return image; +} + + std::vector<uint8_t> ReadBmpImage(const char* filename, int* out_width = nullptr, int* out_height = nullptr, @@ -50,6 +116,9 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, !file.seekg(offset - kBmpHeaderSize, std::ios::cur)) return {}; // Seek failed. + printf(\"kBmpFileHeaderSize:%d\\n\", kBmpFileHeaderSize); + printf(\"kBmpHeaderSize:%d\\n\", kBmpHeaderSize); + printf(\"offset:%d\\n\", offset); int width = ToInt32(&info_header[4]); if (width < 0) return {}; // Invalid width. @@ -63,6 +132,7 @@ std::vector<uint8_t> ReadBmpImage(const char* filename, std::vector<uint8_t> image(line_bytes * height); for (int i = 0; i < height; ++i) { uint8_t* line = &image[(top_down ? i : (height - 1 - i)) * line_bytes]; + printf(\"line_bytes: %d line_padding_bytes:%d\\n\", line_bytes, line_padding_bytes); if (!file.read(reinterpret_cast<char*>(line), line_bytes)) return {}; // Read failed. if (!file.seekg(line_padding_bytes, std::ios::cur)) @@ -117,18 +187,12 @@ std::vector<std::pair<int, float>> Sort(const std::vector<float>& scores, } } // namespace -int main(int argc, char* argv[]) { - if (argc != 5) { - std::cerr << argv[0] - << \" <model_file> <label_file> <image_file> <threshold>\" - << std::endl; - return 1; - } - - const std::string model_file = argv[1]; - const std::string label_file = argv[2]; - const std::string image_file = argv[3]; - const float threshold = std::stof(argv[4]); +extern \"C\" int coral_classify(char *model_file_ptr, char *label_file_ptr, char *image_file_ptr, float thres, char **tags, size_t *out_len) +{ + const std::string model_file (model_file_ptr); + const std::string label_file (label_file_ptr); + const std::string image_file (image_file_ptr); + const float threshold = thres; // Find TPU device. size_t num_devices; @@ -151,7 +215,8 @@ int main(int argc, char* argv[]) { // Load image. int image_bpp, image_width, image_height; auto image = - ReadBmpImage(image_file.c_str(), &image_width, &image_height, &image_bpp); + ReadBmpImageFromBuf(image_file_ptr, &image_width, &image_height, &image_bpp); + //ReadBmpImage(image_file_ptr, &image_width, &image_height, &image_bpp); if (image.empty()) { std::cerr << \"Cannot read image from \" << image_file << std::endl; return 1; @@ -174,7 +239,7 @@ int main(int argc, char* argv[]) { auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0); - interpreter->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate}); + interpreter->ModifyGraphWithDelegate(delegate); // Allocate tensors. if (interpreter->AllocateTensors() != kTfLiteOk) { @@ -204,9 +269,13 @@ int main(int argc, char* argv[]) { // Get interpreter output. auto results = Sort(Dequantize(*interpreter->output_tensor(0)), threshold); + *tags = strdup(GetLabel(labels, results[0].first).c_str()); + *out_len = strlen(*tags); +#if 0 for (auto& result : results) std::cout << std::setw(7) << std::fixed << std::setprecision(5) << result.second << GetLabel(labels, result.first) << std::endl; +#endif return 0; } cd tflite patch -p1 < ../libify_classify.path Alternatively you could just clone from our forked repo: git clone https://github.com/nubificus/tflite -b vaccel Then, we need to install the requirements & build tflite for google coral (could take some time, so please be patient ;-)).","title":"Patch the code"},{"location":"coral/#download-the-latest-edge-tpu-runtime","text":"You can grab the latest Edge TPU runtime archive from https://coral.ai/software/ . Then extract it next to the Makefile: cd tflite/cpp/examples/classification/ wget https://dl.google.com/coral/edgetpu_api/edgetpu_runtime_20200710.zip","title":"Download the latest Edge TPU runtime"},{"location":"coral/#download-and-build-tensorflow","text":"git clone https://github.com/tensorflow/tensorflow.git tensorflow/tensorflow/lite/tools/make/download_dependencies.sh tensorflow/tensorflow/lite/tools/make/build_lib.sh","title":"Download and build Tensorflow"},{"location":"coral/#build-the-shared-library","text":"Finally, build the classify shared object that we will then link to the vAccel Google Coral TPU plugin. g++ -shared -fPIC -std=c++11 classify.cc -o /usr/local/lib/libclassify.so -I/data/tflite/cpp/examples/classification/edgetpu_runtime/libedgetpu/ -Itensorflow/ -Itensorflow//tensorflow/lite/tools/make/downloads/flatbuffers/include -Ltensorflow//tensorflow/lite/tools/make/gen/generic-aarch64_armv8-a/lib -Ledgetpu_runtime/libedgetpu/direct/aarch64/ -l:libedgetpu.so.1.0 -lpthread -lm -ldl tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a","title":"Build the shared library"},{"location":"coral/#build-vaccelrt-plugin","text":"Following the generic process for vAccelRT building (from Building vAccelRT ) we now have to enable the vAccel Google Coral Edge TPU plugin. In short, the previous steps for vAccelRT were the following 1 : git clone --recursive https://github.com/cloudkernels/vaccelrt -b feat_gcoral_plugin cd vaccelrt mkdir build cd build cmake ../ The additional step now is to enable the plugin via cmake: cd vaccelrt mkdir build_coral && cd build_coral cmake ../ -DBUILD_PLUGIN_CORAL=ON The output of the above command should be something like the following: -- The C compiler identification is GNU 8.3.0 -- The CXX compiler identification is GNU 8.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral/googletest-download Scanning dependencies of target googletest [ 11%] Creating directories for 'googletest' [ 22%] Performing download step (git clone) for 'googletest' Cloning into 'googletest-src'... Note: checking out 'release-1.8.1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at 2fe3bd99 Merge pull request #1433 from dsacre/fix-clang-warnings [ 33%] No patch step for 'googletest' [ 44%] Performing update step for 'googletest' [ 55%] No configure step for 'googletest' [ 66%] No build step for 'googletest' [ 77%] No install step for 'googletest' [ 88%] No test step for 'googletest' [100%] Completed 'googletest' [100%] Built target googletest -- Found PythonInterp: /usr/bin/python (found version \"2.7.16\") -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Found rt: /usr/lib/aarch64-linux-gnu/librt.so Include directories: /data/vaccelrt/src -- Configuring done -- Generating done -- Build files have been written to: /data/vaccelrt/build_coral Finally, building the plugin is as simple as issueing a make command: make The plugin is available at: vaccelrt/build_coral/plugins/coral/libvaccel-coral.so","title":"Build vAccelRT plugin"},{"location":"coral/#extra-files","text":"To run an example on the Google Coral Edge TPU, follow the instructions available at Running a simple example . The files needed for classification are: mobilenet_v1_1.0_224_quant_edgetpu.tflite imagenet_labels.txt available for download from: https://github.com/google-coral/test_data . Support for Google Coral is WiP , and will be available shortly in the main branch. \u21a9","title":"Extra Files"},{"location":"jetson/","text":"Jetson-inference vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload with the jetson-inference plugin, we will use a set of NVIDIA GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. The Jetson-inference vAccel plugin is based on jetson-inference , a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. Setup NVIDIA custom repos The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Install CUDA and TensorRT Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Prepare for building jetson-inference Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference Patch to support GTX 1030 & RTX 2060 To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda Building jetson-inference We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install","title":"Jetson-inference"},{"location":"jetson/#jetson-inference","text":"vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload with the jetson-inference plugin, we will use a set of NVIDIA GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. The Jetson-inference vAccel plugin is based on jetson-inference , a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system.","title":"Jetson-inference"},{"location":"jetson/#setup-nvidia-custom-repos","text":"The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update","title":"Setup NVIDIA custom repos"},{"location":"jetson/#install-cuda-and-tensorrt","text":"Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7","title":"Install CUDA and TensorRT"},{"location":"jetson/#prepare-for-building-jetson-inference","text":"Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference","title":"Prepare for building jetson-inference"},{"location":"jetson/#patch-to-support-gtx-1030-rtx-2060","text":"To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda","title":"Patch to support GTX 1030 &amp; RTX 2060"},{"location":"jetson/#building-jetson-inference","text":"We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install","title":"Building jetson-inference"},{"location":"quickstart/","text":"Quickstart This is a quick start guide for running a simple vAccel application on a x86 machine with an Nvidia GPU. We will build the vAccel stack and run a simple application both directly on a Linux host and inside a Firecracker VM. Building the vAccel stack The vAccel is a meta repo which keeps track of all the individual components needed for running a vAccel application. git clone https://github.com/cloudkernels/vaccel cd vaccel The repo includes a build script which will build all the components: The vAccelRT with the currently supported plugins . The virtio-accel kernel module which acts as the transport layer when running inside the Firecracker VM The forked Firecracker VMM which is patched to handle virtio-accel requests A rootfs and suitable vmlinux kernel image for booting the Firecracker VM An example application for testing our setup Building the Jetson inference plugin requires a working jetson-inference installation along with the corresponding CUDA environment on the machine. If you have it you can just run: ./build.sh all Note: while building the rootfs, this script will require your sudo password. If a jetson-inference setup is not available you can use our nubificus/vaccel-deps container image to build the necessary components, by running: ./build.sh -c all Running the application The above build process create two directories: build includes intermediate files of the building process and output includes the build artifacts we will need to run our example: cd output/debug ls bin include lib share Native execution Running on the host In case you have the jetson-inference framework installed on your host, we simply can execute: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Run the image classification example ./bin/classify ./share/images/dog_0.jpg 1 You should some logging from the jetson-inference framework and in the end of it you should get the classification tags of the image: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Note : The first time your run a classification with a model jetson-inference is performing some JIT steps to optimize the classification result, so you can expect increased execution time. The output of this operation is cached for subsequent executions Running in a Firecracker VM build.sh created for us a rootfs image and a vmlinux kernel for booting a Firecracker VM. The rootfs has pre-installed the vAccel libraries, the example application the images and models. Additionally, it installed for us the following Firecracker configuration for booting the VM. In similar fashion as before, we launch the VM: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Launch the Firecracker VM ./bin/firecracker --api-sock fc.sock --config-file share/config_virtio_accel.json --seccomp-level 0 This will launch the Firecracker VM and it will result in a login prompt in the VM, to which we can login using the root user without a password. Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0 vaccel-guest login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Fri Feb 5 17:32:12 UTC 2021 on ttyS0 root@vaccel-guest:~# Once, in the prompt of the guest we setup the environment and run the example: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1 Note : In this case, we use the VirtIO plugin since we are inside the VM and we do not need to define the path to the pre-trained model. The latter is necessary only on the host where we use the jetson plugin. The result of the classification should be the same as before: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Docker execution","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This is a quick start guide for running a simple vAccel application on a x86 machine with an Nvidia GPU. We will build the vAccel stack and run a simple application both directly on a Linux host and inside a Firecracker VM.","title":"Quickstart"},{"location":"quickstart/#building-the-vaccel-stack","text":"The vAccel is a meta repo which keeps track of all the individual components needed for running a vAccel application. git clone https://github.com/cloudkernels/vaccel cd vaccel The repo includes a build script which will build all the components: The vAccelRT with the currently supported plugins . The virtio-accel kernel module which acts as the transport layer when running inside the Firecracker VM The forked Firecracker VMM which is patched to handle virtio-accel requests A rootfs and suitable vmlinux kernel image for booting the Firecracker VM An example application for testing our setup Building the Jetson inference plugin requires a working jetson-inference installation along with the corresponding CUDA environment on the machine. If you have it you can just run: ./build.sh all Note: while building the rootfs, this script will require your sudo password. If a jetson-inference setup is not available you can use our nubificus/vaccel-deps container image to build the necessary components, by running: ./build.sh -c all","title":"Building the vAccel stack"},{"location":"quickstart/#running-the-application","text":"The above build process create two directories: build includes intermediate files of the building process and output includes the build artifacts we will need to run our example: cd output/debug ls bin include lib share","title":"Running the application"},{"location":"quickstart/#native-execution","text":"","title":"Native execution"},{"location":"quickstart/#running-on-the-host","text":"In case you have the jetson-inference framework installed on your host, we simply can execute: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Run the image classification example ./bin/classify ./share/images/dog_0.jpg 1 You should some logging from the jetson-inference framework and in the end of it you should get the classification tags of the image: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute Note : The first time your run a classification with a model jetson-inference is performing some JIT steps to optimize the classification result, so you can expect increased execution time. The output of this operation is cached for subsequent executions","title":"Running on the host"},{"location":"quickstart/#running-in-a-firecracker-vm","text":"build.sh created for us a rootfs image and a vmlinux kernel for booting a Firecracker VM. The rootfs has pre-installed the vAccel libraries, the example application the images and models. Additionally, it installed for us the following Firecracker configuration for booting the VM. In similar fashion as before, we launch the VM: # export the library path export LD_LIBRARY_PATH=$(pwd)/lib # Tell vAccelRT to use the jetson plugin export VACCEL_BACKENDS=./lib/libvaccel-jetson.so # Tell the Jetson plugin where to find the pre-trained models export VACCEL_IMAGENET_NETWORKS=./share/networks # Launch the Firecracker VM ./bin/firecracker --api-sock fc.sock --config-file share/config_virtio_accel.json --seccomp-level 0 This will launch the Firecracker VM and it will result in a login prompt in the VM, to which we can login using the root user without a password. Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0 vaccel-guest login: root Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the 'unminimize' command. Last login: Fri Feb 5 17:32:12 UTC 2021 on ttyS0 root@vaccel-guest:~# Once, in the prompt of the guest we setup the environment and run the example: # export the library path export LD_LIBRARY_PATH=/opt/vaccel/lib # Tell vAccelRT to use the VirtIO plugin export VACCEL_BACKENDS=/opt/vaccel/lib/libvaccel-virtio.so # Launch the Firecracker VM /opt/vaccel/bin/classify images/dog_0.jpg 1 Note : In this case, we use the VirtIO plugin since we are inside the VM and we do not need to define the path to the pre-trained model. The latter is necessary only on the host where we use the jetson plugin. The result of the classification should be the same as before: imagenet: 60.49805% class #249 (malamute, malemute, Alaskan malamute) imagenet: attempting to save output image imagenet: completed saving imagenet: shutting down... classification tags: 60.498% malamute, malemute, Alaskan malamute","title":"Running in a Firecracker VM"},{"location":"quickstart/#docker-execution","text":"","title":"Docker execution"},{"location":"requirements/","text":"System Requirements vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload, we will use a set of NVIDIA 1 GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. Our common vAccel plugin is based on jetson-inference a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install At the time of this writing, support is available for NVIDIA (x86 & aarch64) as well as Intel Iris integrated GPUs and Google Coral Edge TPUs. \u21a9","title":"System Requirements"},{"location":"requirements/#system-requirements","text":"vAccel is a hardware acceleration framework, so, in order to use it the system must have a hardware accelerator and its software stack installed in the Host OS. To walk through the requirements for running a vAccel-enabled workload, we will use a set of NVIDIA 1 GPUs (GTX 1030, RTX 2060 and a Tesla T4) and a common distribution like Ubuntu. Our common vAccel plugin is based on jetson-inference a frontend for TensorRT, developed by NVIDIA. This intro section will serve as a guide to install TensorRT, CUDA and jetson inference on a Ubuntu 18.04 system. The first step is to prepare the package system for NVIDIA's custom repos: # install common utils & get NVIDIA's key apt-get update && apt-get install -y --no-install-recommends gnupg2 curl ca-certificates wget sudo curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/7fa2af80.pub | apt-key add - # download repo pkgs and install them wget https://developer.download.nvidia.com/compute/cuda/repos/${os}/x86_64/cuda-repo-${os}_${cuda}-1_amd64.deb wget https://developer.download.nvidia.com/compute/machine-learning/repos/${os}/x86_64/nvidia-machine-learning-repo-${os}_1.0.0-1_amd64.deb dpkg -i cuda-repo-*.deb dpkg -i nvidia-machine-learning-repo-*.deb # update pkg list apt-get update Next, we need to install CUDA and TensorRT: # install TensorRT DEBIAN_FRONTEND=noninteractive apt-get install -yy eatmydata && \\ DEBIAN_FRONTEND=noninteractive eatmydata apt-get install -y cuda-11-1 libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 Following the successful installation of TensorRT, we need to install the requirements for the jetson-inference build system: apt-get update && apt-get install -y git \\ libnvinfer-dev \\ libnvinfer-plugin-dev \\ libpython3-dev \\ pkg-config \\ python3-libnvinfer-dev \\ python3-numpy Finally, we clone jetson-inference: git clone --recursive https://github.com/nubificus/jetson-inference cd jetson-inference To support GTX 1030 and RTX 2060 we need to patch the source tree to add the relevant compute versions ( Enable-CM-60-75.patch ): diff --git a/CMakeLists.txt b/CMakeLists.txt index 46c997dd..d4c3a56c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -60,16 +60,18 @@ set( ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_53,code=sm_53 + -gencode arch=compute_60,code=sm_60 -gencode arch=compute_62,code=sm_62 ) if(CUDA_VERSION_MAJOR GREATER 9) - message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72\") + message(\"-- CUDA ${CUDA_VERSION_MAJOR} detected, enabling SM_72 and SM75\") set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72 + -gencode arch=compute_75,code=sm_75 ) endif() patch -p1 < Enable-CM-60-75.patch Then, let's make sure we have a proper compiler & Cmake installed: TZ=Europe/London ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone apt-get install -y cmake build-essential Jetson-inference assumes CUDA is installed in /usr/local/cuda. There is a possibility that cuda will be installed in a folder using the version numbering so to make sure the build system can find it we do the following: ln -sf /usr/local/cuda-11.1 /usr/local/cuda We create a build dir, enter it prepare the Makefiles: mkdir build cd build BUILD_DEPS=YES cmake -DBUILD_INTERACTIVE=NO ../ Finally, we issue the build command and we install it to our system: make -j$(nproc) make install At the time of this writing, support is available for NVIDIA (x86 & aarch64) as well as Intel Iris integrated GPUs and Google Coral Edge TPUs. \u21a9","title":"System Requirements"},{"location":"run/","text":"Running a simple example Building a vaccel application We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images Running the example Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: $ ls classify images networks $ VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running a simple example"},{"location":"run/#running-a-simple-example","text":"","title":"Running a simple example"},{"location":"run/#building-a-vaccel-application","text":"We will use an example of image classification which can be found under the examples folder of the vAccel runtime repo . You can build the example using the CMake of the repo: $ mkdir build $ cd build $ cmake -DBUILD_EXAMPLES=ON .. $ make $ ls examples classify CMakeFiles cmake_install.cmake Makefile If, instead, you want to build by hand you need to define the include and library paths (if they are not in your respective default search paths) and also link with dl : $ cd ../examples $ gcc -Wall -Wextra -I${HOME}/.local/include -L${HOME}/.local/lib classification.c -o classify -lvaccel -ldl $ ls classification.c classify CMakeLists.txt images","title":"Building a vaccel application"},{"location":"run/#running-the-example","text":"Having built our classify example, we need to prepare the vaccel environment for it to run: Define the path to libvaccel.so (if not in the default search path): export LD_LIBRARY_PATH=${HOME}/.local/lib Define the backend plugin to use for our application. In this example, we will use the jetson plugin which implements the image classification operation using the Jetson Inference framework which uses TensorRT. export VACCEL_BACKENDS=${HOME}/.local/lib/libvaccel-jetson.so Finally, the classification application needs the imagent models in the current working path. (TODO: Find link to download those). Once you have these, you can do: $ ls classify images networks $ VACCEL_IMAGENET_NETWORKS=$(pwd) ./classify images/banana_0.jpg 1 Initialized session with id: 1 Image size: 79281B classification tags: 99.902% banana","title":"Running the example"},{"location":"vaccelrt/","text":"","title":"Vaccelrt"},{"location":"k8s/kata/","text":"vAccel on k8s using Kata & Firecracker TODO - add proper links to yaml files - add correct outputs - add components build documentation Prerequisites In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter NVIDIA GPU which supports CUDA ( for now ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries) Quick start We built on kata-containers/kata-deploy to deploy vAccel on Kata Containers. Our fork repo can be found on cloudkernels/packaging Deploy vAccel with Kata First label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m default web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 76m default web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 76m ... ... $ k3s kubectl -n kube-system logs kata-deploy-575tm ... ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel **warning: containerd-shim-kata-fc-v2 already exists** node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before (~2GB). $ kubectl kata-deploy-full.yaml Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml Deploy an image classification function as a Service The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl web-classify.yaml NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ k3s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST classify.nbfc.io/classify --data-binary @-","title":"vAccel on k8s using Kata & Firecracker"},{"location":"k8s/kata/#vaccel-on-k8s-using-kata-firecracker","text":"TODO - add proper links to yaml files - add correct outputs - add components build documentation","title":"vAccel on k8s using Kata &amp; Firecracker"},{"location":"k8s/kata/#prerequisites","text":"In order to run vAccel on Kata containers with Firecracker you need to meet the following prerequisites on each k8s node that will be used for acceleration: containerd as container manager devicemapper as CRI plugin default snapshotter NVIDIA GPU which supports CUDA ( for now ) jetson-inference libraries (libjetson-inference.so must be installed and properly linked with CUDA libraries)","title":"Prerequisites"},{"location":"k8s/kata/#quick-start","text":"We built on kata-containers/kata-deploy to deploy vAccel on Kata Containers. Our fork repo can be found on cloudkernels/packaging","title":"Quick start"},{"location":"k8s/kata/#deploy-vaccel-with-kata","text":"First label each node where vAccel-kata should be deployed: $ kubectl label nodes <your-node-name> vaccel=true Install vAccel-kata on each \"vaccel=true\" node: $ kubectl apply -f https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/kata-deploy/base/kata-deploy.yaml The kata-deploy daemon calls the vAccel download script. It may take a few minutes to download the ML Inference models. Check the pod logs to be sure that the installation is complete. You should see something like the following: $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kata-deploy-575tm 1/1 Running 0 101m default web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 76m default web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 76m ... ... $ k3s kubectl -n kube-system logs kata-deploy-575tm ... ... ... Done! containerd-shim-kata-v2 is now configured to run Firecracker with vAccel **warning: containerd-shim-kata-fc-v2 already exists** node/node3.nubificus.com labeled That's it! You are now ready to accelerate your functions on Kubernetes with vAccel. Alternatively use the following daemon which already contains all the vAccel artifacts and required components in the container image. The image is slightly bigger than before (~2GB). $ kubectl kata-deploy-full.yaml Don't forget to create a RuntimeClass in order to run your workloads with vAccel enabled kata runtime $ kubectl apply https://raw.githubusercontent.com/cloudkernels/packaging/vaccel-dev/kata-deploy/k8s-1.14/kata-fc-runtimeClass.yaml","title":"Deploy vAccel with Kata"},{"location":"k8s/kata/#deploy-an-image-classification-function-as-a-service","text":"The following will deploy a custom HTTP server that routes POST requests to a handler. The handler gets an image from the POST body and calls vAccel to perform image-classification operation using the GPU. $ kubectl web-classify.yaml NAME READY STATUS RESTARTS AGE web-classify-kata-fc-5f44fd448f-mtvlv 1/1 Running 0 92m web-classify-kata-fc-5f44fd448f-h7j84 1/1 Running 0 92m $ k3s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web-classify-kata-fc ClusterIP 10.43.214.52 <none> 80/TCP 91m $ wget https://pbs.twimg.com/profile_images/1186928115571941378/1B6zKjc3_400x400.jpg -O - | curl -L -X POST classify.nbfc.io/classify --data-binary @-","title":"Deploy an image classification function as a Service"},{"location":"unikernels/unikraft/","text":"In this post we will describe the process of running an image classification app in Unikraft using vAccel. In order to do that we gonna need; a NVIDIA GPU which supports CUDA the appropriate drivers and jetson-inference As long as we have the above depedencies, we can move on setting up the vAccel environment for our example. We will need to set up 3 components: vAccelrt on the bost A hypervisor with vAccel suuport, in our case QEMU and of course Unikraft To make things easier we have created some scripts and Docketfiles which produce a vAccel enabled QEMU and a Unikraft unikernel. You can get them from this repo. The first 2 components can be built using the build.sh script with the -u option, like: bash build.sh -u The last component can be created using the build_guest.sh script with the -u option, like: bash build_guest.sh -u After succefully building all components we can just fire up our unikernel using the command: bash run.sh This command will start Unikraft over QEMU classifying the image dog_0.jpg under guest/data/ directory of this repo. Let's take a closer look on what is happening inside the containers and how we can do each step by ourselves. Setting up vAccert for the host vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case this hardware will be a NVIDIA GPU. As a result we need to build the Jetson plugin for vAccelrt too. At first let's get the source code git clone https://github.com/cloudkernels/vaccelrt.git cd vaccelrt git submodule update --init As we said before we will also build the jetson plugin and we will speccify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so Build QEMU with vAccel support As always at first we need to get the source code. For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. git clone https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel git submodule update --init We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side. Build the Unikraft application We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but out fork. In out fork there is also an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages.. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under build direcotry. Fire it up After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run out unikernel. LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"How to run a vAccel application using Unikraft"},{"location":"unikernels/unikraft/#setting-up-vaccert-for-the-host","text":"vAccelrt is a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. In our case this hardware will be a NVIDIA GPU. As a result we need to build the Jetson plugin for vAccelrt too. At first let's get the source code git clone https://github.com/cloudkernels/vaccelrt.git cd vaccelrt git submodule update --init As we said before we will also build the jetson plugin and we will speccify ~/.local/ as the directory where vAccelrt will be installed. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=/.local -DBUILD_PLUGIN_JETSON=ON .. make && make Install Now that we have vAccelrt installed we need to set the plugin that we will use export VACCEL_BACKENDS=/.local/lib/libvaccel-jetson.so","title":"Setting up vAccert for the host"},{"location":"unikernels/unikraft/#build-qemu-with-vaccel-support","text":"As always at first we need to get the source code. For the time being we have a seperated branch where QEMU exposes vAccel to the guest with legacy virtio. git clone https://github.com/cloudkernels/qemu-vaccel.git -b vaccelrt_legacy_virtio cd qemu-vaccel git submodule update --init We will configure QEMU with only one target and we need to point out the install location of vAccelrt with cflags and ldflags. Please set the install prefix in case you do not want it to be installed in the default directory. mkdir build && cd build ../configure --extra-cflags=\"-I /.local/include\" --extra-ldflags=\"-L/.local/lib\" --target-list=x86_64-softmmu --enable-virtfs make -j12 && make install THat's it. We are done with the host side.","title":"Build QEMU with vAccel support"},{"location":"unikernels/unikraft/#build-the-unikraft-application","text":"We will follow the instructions from Unikraft's documentation with a minor change. We will not use the official Unikraft repo but out fork. In out fork there is also an example application for image classification. git clone https://github.com/cloudkernels/unikraft.git mkdir apps/ cp -r unikraft/example apps/classify cd apps/classify Our example has a config which enables vAccel support for Unikraft and a lot of debug messages.. You can use that config or you can make your own config, but make sure you select vaccelrt under library configuration inside menuconfig. cp vaccel_config .config && make olddefconfig make After building is done, there will be a Unikraft image for KVM under build direcotry.","title":"Build the Unikraft application"},{"location":"unikernels/unikraft/#fire-it-up","text":"After that long journey of building let's see what we have done. Not yet... One more thing. We need to get the model which will be used for the image classification. We will use a script which is already provided by jetson-inference (normally under `/usr/local/share/jetson-inference/tools. Also we need to point vAccel's jetson plugin the path to that model. /path/to/download-models.sh cp /usr/local/share/jetson-inference/data/networks/* networks export VACCEL_IMAGENET_NETWORKS=/full/path/to/newly/downloaded/networks/directory Finally we can run out unikernel. LD_LIBRARY_PATH=/.local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 qemu-system-x86_64 -cpu host -m 512 -enable-kvm -nographic -vga none \\ -fsdev local,id=myid,path=/data/data,security_model=none -device virtio-9p-pci,fsdev=myid,mount_tag=data,disable-modern=on,disable-legacy=off \\ -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on \\ -kernel /data/classify_kvm-x86_64 -append \"vfs.rootdev=data -- dog_0.jpg 1\" Let's highlight some parts of the qemu command: LD\\_LIBRARY\\_PATH environment variable: QEMU will dynamically link with every library it needs and in this case it also needs the vAccelrt library -fsdev local,id=myid,path=./data,security\\_model=none : We need to tell QEMU where will find the data that will pass to the guest. The data directory contains the image we want to classify -object acceldev-backend-vaccelrt,id=gen0 -device virtio-accel-pci,id=accl0,runtime=gen0,disable-legacy=off,disable-modern=on : For the vAccel driver -append \"vfs.rootdev=data -- dog_0.jpg 1\" : In the command line we need to tell the classify application which image to classify and how many iterations to do","title":"Fire it up"}]}